# 爬虫入门

- 爬虫的定义
- 爬虫的分类
- 爬虫的流程
- 爬虫相关的http和https
- requests模块的使用
- lxml模块的使用



# 1 爬虫基础

## 1.1 定义和使用场景

**什么是网络爬虫**

模拟客户端发送网络请求，接收请求响应, 提取数据的自动化程序.


**获取数据的途径**

- 第三方公司 (比如企查查)
- 免费的数据网站 (比如国家统计局)
- 爬虫爬取数据
- 人工收集数据 (比如问卷调查)

**爬虫获取的数据的用途**

- 在网页或者是app上进行展示
- 进行数据分析或者是机器学习相关的项目



## 1.2 爬虫的分类和流程

### 分类

根据被爬网站的数量的不同，把爬虫分为：

- 通用爬虫 ：通常指搜索引擎的爬虫
- 聚焦爬虫 ：针对特定网站的爬虫

### 流程

种子url -- > 发送请求，获取响应---->提取数据--->保存
获取响应 --> 提取url地址，继续请求

![爬虫工作流程](.\01_爬虫入门_images\爬虫工作流程.png)

**搜索引擎的局限性**

- 通用搜索引擎所返回的网页里90%的内容无用
- 图片、音频、视频多媒体的内容通用搜索引擎无能为力
- 不同用户搜索的目的不全相同，但是返回内容相同

### robots协议

主要针对 `通用爬虫 (搜索引擎爬虫)`

Robots协议：网站通过Robots协议告诉 搜索引擎  哪些页面可以抓取，哪些页面不能抓取，但它仅仅是 *道德层面上的约束*
查看网站的robots协议:  `https://xxx.xxx.xxx/robots.txt`



## 1.3 HTTP和HTTPS

### 概念

- HTTP
  - 超文本传输协议
  - 默认端口号: 80
- HTTPS
  - HTTP(超文本传输协议) + SSL(安全套接字层)
  - 默认端口号：443

HTTPS比HTTP更安全，但是性能更低



### 浏览器发送HTTP请求的过程

![http发送的过程](.\01_爬虫入门_images\http发送的过程.png)



浏览器会主动请求js，css等内容，js会修改页面的内容，js也可以重新发送请求，最后浏览器渲染出来的内容在elements中，其中包含css，图片，js，url地址对应的响应等。

**但在爬虫中，爬虫只会请求url地址，对应的拿到url地址对应的响应,**
浏览器渲染出来的页面和爬虫请求的页面 **并不一样**,   
所以在爬虫中，需要 **以 url 地址对应的响应** 为准来进行数据的提取,  不能以浏览器elements为准.



### url 的组成

url的形式：`scheme://host[:port#]/path/…/[?query-string][#anchor]`

- scheme：协议类型(例如：http, https, ftp)
- host：服务器的IP地址或者域名
- port：服务器的端口（如果是走协议默认端口，80 or 443）
- path：访问资源的路径
- query-string：查询字符串参数，发送给http服务器的数据
- anchor：锚（跳转到网页的指定锚点位置）
  - http://localhost:4000/file/part01/1.2.html
  - http://item.jd.com/11936238.html#product-detail
  - **url地址中是否包含锚点对响应没有影响**




### HTTP的请求形式

请求行:	请求方法 + url + 协议版本
请求头:	headers
请求体:	body


**http的重点请求头**

- `User-Agent`:告诉对方服务器是什么客户端正在请求资源，是爬虫中模拟浏览器非常重要的一个手段,

  **用户代理, 浏览器信息,  服务器可以根据User-Agent区分app端/pc端/爬虫,   请求同一个url, 返回不同种页面**

- `Cookie`：获取只有登录才能够访问的资源




### 比较 GET 与 POST

get请求和post请求的区别可以参 [w3school](http://www.w3school.com.cn/tags/html_ref_httpmethods.asp)

| 对比项               | GET                                                          | POST                                                         |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 后退按钮/刷新        | 无害                                                         | 数据会被重新提交（浏览器应该告知用户数据会被重新提交）。     |
| 书签                 | 可收藏为书签                                                 | 不可收藏为书签                                               |
| 缓存                 | 能被缓存                                                     | 不能缓存                                                     |
| 编码类型             | application/x-www-form-urlencoded                            | application/x-www-form-urlencoded 或 multipart/form-data。为二进制数据使用多重编码。 |
| 历史                 | 参数保留在浏览器历史中。                                     | 参数不会保存在浏览器历史中。                                 |
| **对数据长度的限制** | 有限制。当发送数据时，GET 方法向 URL 添加数据；URL 的长度是受限制的（**URL 的最大长度是 2048 个字符**）。 | 无限制。                                                     |
| 对数据类型的限制     | 只允许 ASCII 字符。                                          | 无限制。也允许二进制数据。                                   |
| **安全性**/可见性    | 与 POST 相比，GET 的安全性较差，因为所发送的数据是 URL 的一部分。在发送密码或其他敏感信息时绝不要使用 GET ！数据在 URL 中对所有人都是可见的 | POST 比 GET 更安全，因为参数不会被保存在浏览器历史或 web 服务器日志中。数据不会显示在 URL 中 |



**HTTP常见请求头**

```python
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8							传输文件类型, q: 权重
Accept-Encoding: gzip, deflate, br		接收的编码方式
Accept-Language: zh-CN,zh;q=0.9			接收的语言
Cache-Control: max-age=0				缓存控制
Connection: keep-alive					链接类型: 长链接
Cookie: key1=value1;key2=value2;... 
Host: blog.csdn.net						主机和端口号
Referer: https://www....				页面跳转处,  可判断Referer用来反爬
Upgrade-Insecure-Requests: 1			升级为HTTPS请求
User-Agent: Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.79 Safari/537.36  		浏览器名称, 电脑版User-Agent
```

**常见的响应状态码：**

- 200：成功
- 302：临时转移至新的url
- 307：临时转移至新的url
- 404：not found
- 500：服务器内部错误





## 1.4 字符串编码

### Unicode、UTF8、ASCII

字符(Character)	是各种文字和符号的总称，包括各国家文字、标点符号、图形符号、数字等

字符集(Character set)	是多个字符的集合

字符集包括：ASCII字符集、GB2312字符集、GB18030字符集、Unicode字符集等

ASCII编码	是1个字节，而Unicode编码通常是2个字节。

UTF-8		是Unicode的实现方式之一，UTF-8是一种**变长的编码方式**，可以是1，2，3个字节

了解更多：[ascii和unicode以及utf-8的起源](https://blog.csdn.net/kun1280437633/article/details/80639487)



### python2、3中的字符串

ascii 	一个字节表示一个字符
unicode 	两个字节表示一个字符
utf-8 	可变长度的编码方式，1，2，3字节表示一个字符

- python2
  - 字节类型：str， ------decode()----->   unicode类型
  - unicode类型：unicode ， ------encode()----->   str字节类型
- python3
  - unicode：str， ------encode()----->   bytes

    `u"abc".encode() -----> b'abc' `

  - 字节类型：bytes， -------decode()----->   str类型


    b"abc".decode() -----> 'abc'


##    


# 2 requests模块的使用

中文文档 ： http://docs.python-requests.org/zh_CN/latest/index.html



**requests.get()方法的部分参数**

```python
url				请求url
params 			查询字符串参数,  Dictionary or bytes
data			请求体参数,  Dictionary, bytes, or file-like object
json			请求体 json 数据
headers			请求头字典
cookies			字典或CookieJar对象
files: 			字典类型, 传输文件
auth: 			元组，支持HTTP认证功能
timeout: 		设置超时时间, s
allow_redirects	重定向开关, 默认为True
stream:			获取内容立即下载开关,  默认为True
proxies: 		添加代理, 字典
verify: 		是否开启SSL证书认证, 默认为True
cert: 			本地SSL证书路径
```





## 2.1 基础用法

requests模块与urllib3

- requests的底层实现就是urllib3
- requests在python2 和python3中通用，方法完全一样
- Requests能够自动解压(gzip压缩的等) 网页内容



### 发起请求, 接收响应对象	

```response = requests.get(url)```

响应对象常用属性	

```
 response.text
 response.content
 response.status_code
 response.request.headers
 response.request.url
 response.headers
```



**response.text 和response.content的区别**

- `response.text`
  - 返回类型：str
  - 解码类型： 自动根据HTTP 头部对响应的编码作出有根据的推测，推测的文本编码,  **不一定推测正确**
  - 如何修改编码方式：先设置编码方式  `response.encoding=”gbk”`,   再获取响应数据 `response.text`
- `response.content`
  - 返回类型：bytes
  - 解码类型： 没有指定
  - 如何修改编码方式：`response.content.deocde(“utf8”)`

**获取网页源码的通用方式：**

1. `response.content.decode()`	默认按“utf8”解码
2. `response.content.decode("GBK")`
3. `response.text`

以上三种方法从前往后尝试，能够**100%**的解决所有网页解码的问题

更推荐使用`response.content.deocde()`的方式获取响应的html页面



### 自定义请求头headers

**思考**: 
对比浏览器上百度首页的网页源码 和 代码中的百度首页的源码，有什么不同？
代码中的百度首页的源码非常少，为什么？   (遇到反爬措施之一:  识别User-Agent为爬虫)



### 爬取贴吧html

**面向过程**

```python
import requests


headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.79 Safari/537.36',
}

# 构造 url 列表
kw = input("[kw]:")
url_temp = 'https://tieba.baidu.com/f?kw={kw}&ie=utf-8&pn={pn}'
pn_list = [pn * 50 for pn in range(0, 1001)]
url_list = list()
for pn in pn_list:
    url_list.append(url_temp.format(kw=kw, pn=pn))
print(url_list)


# 发起请求
def get_content(url):
    response = requests.get(url, headers=headers)
    content = response.content.decode()
    with open('./contents/' + url[url.find('pn')+3:], 'w') as f:
        f.write(content)


for url in url_list:
    get_content(url)
```



**面向对象**

```python
import requests


# https://tieba.baidu.com/f?kw=李毅&ie=utf-8&pn=50
class Spiders(object):
    """
    贴吧爬虫类
    """
    def __init__(self, tieba_name):
        self.tieba_name = tieba_name
        self.url_temp = 'https://tieba.baidu.com/f?kw=' + self.tieba_name + '&ie=utf-8&pn={}'
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.79 Safari/537.36',
        }

    def get_url_list(self):
        """
        构造url列表
        """
        return [self.url_temp.format(i*50) for i in range(301)]

    def parse_url(self, url):
        """
        发起请求
        """
        response = requests.get(url, headers=self.headers)
        return response.content.decode()

    def save_html(self, html_str, page_num):
        """
        保存
        """
        file_path = './contents/{}吧-第{}页.html'.format(self.tieba_name, page_num)
        with open(file_path, 'w') as f:
            f.write(html_str)

    def run(self):
        """
        爬虫主程序
        """
        # 获取url列表
        url_list = self.get_url_list()

        for url in url_list:
            # 遍历URL列表, 发起请求, 接收响应
            html_str = self.parse_url(url)

            # 保存html
            page_num = url_list.index(url) + 1
            self.save_html(html_str, page_num)
            print("\r%s..." % url, end='')
        print('----END----')


if __name__ == '__main__':
    spiders = Spiders('李毅')
    spiders.run()
```





## 2.2 高级用法

### 发送POST请求

**哪些地方会用到POST请求：**

- 登录注册（ POST 比 GET 更安全）
- 需要传输大文本内容的时候（ POST 请求对数据长度没有要求）

所以同样的，我们的爬虫也需要在这种情况下去模拟浏览器发送post请求

- 用法： `response = requests.post("http://www.baidu.com/", data=data, headers=headers)`
- data 的形式：字典




###> 百度翻译post

- 面向对象编程
- (爬取前分析) 以post方式请求翻译,  携带参数可以从浏览器开发者工具分析找到
- `todo`:  (爬取前分析)  需要模拟手机浏览器,   使用手机浏览器用户代理  User-Agent  爬取百度翻译结果,  因为电脑端在请求时需要带上变化的 sign 和 token,  而手机端不需要
- 以命令行模式执行,  提取命令行参数 (`sys.argv[1]`)
- Linux终端使用 `alias` 自定义终端命令的步骤
- json模块将响应json转换为字典dict,  提取翻译结果




### > alias 终端命令别名

- 编辑 `~/.zshrc`  或者  `~/.bashrc`,   这个文件主要保存个人的一些个性化设置,  如命令别名,  路径...
- 编辑成功后,  使生效:  `source ~/.zshrc`

![alias可以自定义终端命令](.\01_爬虫入门_images\alias可以自定义终端命令.png)

![alias自定义终端命令的步骤](.\01_爬虫入门_images\alias自定义终端命令的步骤.png)



**百度翻译流程:**

1  获取 2个语言类型 + 翻译内容

2  请求语言检测的接口: 
​	请求地址:	https://fanyi.baidu.com/langdetect
​	请求方式: 	POST
​	请求参数: 	 (FormData)   query: '翻译内容'
​	返回值: 		json--->{"error":0,"msg":"success","lan":"zh"},  从其中提取语言类型

3  请求基础翻译的接口:
​	请求地址: 	https://fanyi.baidu.com/basetrans(手机端翻译url)
​	请求方式: 	POST
​	请求参数:  	(FormData)   query: 翻译内容,  from: zh,  to: en
​	返回值: 		json,  从其中提取基础翻译结果

4  请求翻译拓展的接口:
​	请求地址: 	https://fanyi.baidu.com/extendtrans(手机端翻译url)
​	请求方式: 	POST
​	请求参数:  	(FormData)   query: 翻译内容,  from: zh,  to: en
​	返回值: 		json,  从其中提取翻译拓展结果



```python
import sys
import json
import requests


class BaiDuTranslate(object):
    """
    百度翻译类----基础翻译
    """

    def __init__(self, query_string):
        self.url = 'https://fanyi.baidu.com/basetrans'
        self.query_string = query_string
        self.headers = {
            # 手机浏览器的用户代理
            'User-Agent': 'Mozilla/5.0 (Linux; Android 5.1.1; Nexus 6 Build/LYZ28E) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Mobile Safari/537.36',
        }

    def get_post_data(self):
        """
        组织表单数据
        """
        post_data = {
            "from": "zh",
            "to": "en",
            "query": self.query_string,
            # "transtype": "translang",
            # "simple_means_flag": 3,
            # "sign": "440615.154134",
            # "token": "555ed5190c87fe757bda14911d783a1c",
        }
        return post_data

    def parse_url(self, url, post_data):
        """
        发起请求, 接收响应
        """
        response = requests.post(url, data=post_data, headers=self.headers)
        return response.content.decode()

    def get_result(self, json_str):
        json_dict = json.loads(json_str)
        result = json_dict['trans'][0]['dst']
        print('翻译结果是: {}'.format(result))

    def run(self):
        """
        百度翻译主程序
        """
        # url,  post_data
        post_data = self.get_post_data()
        # 发起请求, 接收响应
        json_str = self.parse_url(self.url, post_data)
        # 提取json数据
        self.get_result(json_str)


if __name__ == '__main__':
    # 提取命令行参数
    # query_string = sys.argv[1]
    query_string = ''.join(sys.argv[1:])

    # query_string = "人生苦短, 我用python"
    baidu_translate = BaiDuTranslate(query_string)
    baidu_translate.run()
```



### > 百度翻译_中英文检测

- 提取的命令行参数,  向检测语言类型的url发起请求,  接收检测结果并提取语言类型,  判断结果,  根据检测出的中/英文结果向翻译url发起请求,  分别携带不同参数

```python
import sys
import json
import requests


class BaiDuTranslate(object):
    """
    百度翻译类----基础翻译----可以检测语言类型
    todo 模拟手机浏览器使用百度翻译, 因为电脑端在请求时需要带上变化的token, 手机端不需要
    """

    def __init__(self, query_string):
        self.lang_detect_url = 'https://fanyi.baidu.com/langdetect'
        self.url = 'https://fanyi.baidu.com/basetrans'
        self.query_string = query_string
        self.headers = {
            # todo 手机浏览器的用户代理
            'User-Agent': 'Mozilla/5.0 (Linux; Android 5.1.1; Nexus 6 Build/LYZ28E) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Mobile Safari/537.36',
        }

    def get_post_data(self):
        """
        组织表单数据
        """
        # 获取中英文检测结果
        detect_lan = self.lang_detect()
        post_data = {
            "from": "zh" if detect_lan == "zh" else 'en',
            "to": "en" if detect_lan == "zh" else 'zh',
            "query": self.query_string,
            # "transtype": "translang",
            # "simple_means_flag": 3,
            # "sign": "440615.154134",
            # "token": "555ed5190c87fe757bda14911d783a1c",
        }
        return post_data

    def lang_detect(self):
        data = {
            "query": self.query_string
        }
        ret_str = requests.post(self.lang_detect_url, data=data, headers=self.headers)
        ret_json = ret_str.content.decode()
        ret_dict = json.loads(ret_json)
        detect_lan = ret_dict['lan']  # {"error":0,"msg":"success","lan":"zh"}
        return detect_lan

    def parse_url(self, url, post_data):
        """
        发起请求, 接收响应
        """
        response = requests.post(url, data=post_data, headers=self.headers)
        return response.content.decode()

    def get_result(self, json_str):
        json_dict = json.loads(json_str)
        result = json_dict['trans'][0]['dst']
        print('翻译结果是: {}'.format(result))

    def run(self):
        """
        百度翻译主程序
        """
        # url,  post_data
        post_data = self.get_post_data()
        # 发起请求, 接收响应
        json_str = self.parse_url(self.url, post_data)
        # 提取json数据
        self.get_result(json_str)


if __name__ == '__main__':
    # 提取命令行参数
    # query_string = sys.argv[1]
    query_string = ' '.join(sys.argv[1:])

    # query_string = "人生苦短, 我用python"
    baidu_translate = BaiDuTranslate(query_string)
    baidu_translate.run()
```





### 使用代理ip

**为什么要使用代理**

- 让服务器以为不是同一个客户端在请求
- 防止真实ip被泄露，防止被追究

**使用代理的 请求和响应过程**

![使用代理的过程](.\01_爬虫入门_images\使用代理的过程.png)



**正向代理和反向代理的区别**

![正向代理和反向代理的区别](.\01_爬虫入门_images\正向代理和反向代理的区别.png)

` 正向代理`：对于浏览器知道服务器的真实地址，例如 VPN 

`反向代理`：浏览器不知道服务器的真实地址，例如 nginx



**代理ip的使用**

- 用法： `requests.get("http://www.baidu.com", proxies=proxies)`
- 访问http协议的url需要用HTTP代理,   访问https协议的url需要用HTTPS代理
- 部分代理 ip 支持post请求

```python
proxies = {
	# {"协议": "协议://IP:port", }
    "http": "http://12.34.56.79:9527", 
    "https": "https://12.34.56.79:9527", 
    }
```



**代理IP的分类**

根据代理服务器端的配置，向目标地址发送请求时，`REMOTE_ADDR`， `HTTP_VIA`，`HTTP_X_FORWARDED_FOR` 三个变量不同而可以分为下面 4 类：

- 透明代理(Transparent Proxy)

  ```python
    REMOTE_ADDR = Proxy IP
    HTTP_VIA = Proxy IP
    HTTP_X_FORWARDED_FOR = Your IP
  ```

  透明代理虽然可以直接“隐藏”你的IP地址，但是还是可以从HTTP_X_FORWARDED_FOR来查到你是谁。

- 匿名代理(Anonymous Proxy)

  ```python
    REMOTE_ADDR = proxy IP
    HTTP_VIA = proxy IP
    HTTP_X_FORWARDED_FOR = proxy IP
  ```

  匿名代理比透明代理进步了一点：别人只能知道你用了代理，无法知道你是谁。


- 混淆代理(Distorting Proxies)

  ```python
    REMOTE_ADDR = Proxy IP
    HTTP_VIA = Proxy IP
    HTTP_X_FORWARDED_FOR = Random IP address
  ```

  如上，与匿名代理相同，如果使用了混淆代理，别人还是能知道你在用代理，但是会得到一个假的IP地址，伪装的更逼真

- **高匿代理** (Elite proxy或High Anonymity Proxy)

  ```python
    REMOTE_ADDR = Proxy IP
    HTTP_VIA = not determined
    HTTP_X_FORWARDED_FOR = not determined
  ```

  可以看出来，高匿代理让别人根本无法发现你是在用代理，所以是最好的选择。

从使用的协议：代理ip可以分为 `http代理`，`https代理`，`socket代理` 等，使用的时候需要根据抓取网站的协议来选择



**代理IP使用的注意点**

- 反反爬

  使用代理ip是非常必要的一种`反反爬`的方式,  但是即使使用了代理ip，对方服务器任然会有很多的方式来检测我们是否是一个爬虫

  比如：

  - 一段时间内，检测IP访问的频率，访问太多频繁则屏蔽

  - 检查Cookie，User-Agent，Referer等header参数，若没有则屏蔽

  - 服务方购买所有代理提供商，加入到反爬虫数据库里，若检测是代理则屏蔽

    所以更好的方式是购买质量更高的代理，或者自己搭建代理服务器，组装自己的 `代理IP池`，同时在使用的时候使用随机的方式进行选择使用，不要每次都用一个代理ip

- 代理ip池的更新

  购买的代理ip很多时候大部分(超过60%)可能都没办法使用，这个时候就需要通过程序去检测哪些可用，把不能用的删除掉。对应的实现方式在学习了`超时参数的使用`之后会了解

- 面试

  很多时候面试官会了解自己项目中的代理ip的使用情况，比如：代理ip池的大小，代理ip的更新时间，代理ip中可用的代理ip的比例等等，这些都是开放性的问题. 




## 2.3 处理cookie相关的请求

### 处理cookie相关的请求

**cookie和session的区别**

- cookie数据存放在客户的浏览器上，session数据放在服务器上。
- cookie不是很安全，别人可以分析存放在本地的cookie并进行cookie欺骗。
- session会在一定时间内保存在服务器上。当访问增多，会比较占用服务器的性能。
- 单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie。



**爬虫中为什么要使用cookie**

- 带上cookie的好处
  - 能够访问登录后的页面
  - 正常的浏览器在请求服务器的时候肯定会带上cookie（第一次请求某个地址除外），所以对方服务器有可能会通过是否携带相关cookie来判断我们是否是一个爬虫，增加反爬措施
- 带上cookie的坏处
  - 一套cookie往往对应的是一个用户的信息，请求太频繁有更大的可能性被对方识别为爬虫
  - 那么上面的问题如何解决?  使用多个账号:   `cookie池`




>  处理cookie的三种方法:

### >1 requests.Session类 处理cookie

- requests 提供了一个叫做Session类，来实现客户端和服务端的`会话保持`
- 会话保持有两个内涵：
  - **保存cookie**
  - 实现和服务端的**长连接**


`requests.Session类 的作用:`

使用Session的实例请求了一个网站后，对方服务器设置在本地的cookie会保存在实例session中，下一次再使用实例session请求对方服务器的时候，会自动带上前一次的cookie

- **使用方法**
  ```python
  # 1. 实例化Session对象
  session = requests.Session()

  # 组织登录表单数据
  post_data = {
      'email': '18770915328',
      'password': 123456
  }

  # 2. 第一次使用session对象发起登录post请求, 带上用户名密码等参数, 请求成功后session对象会保存cookie
  resp1 = session.post(post_url, data=post_data, headers=headers)
  ...

  # 3. 再使用session对象发起请求,  会自动携带前面的cookie
  resp2 = session.get(url, headers=headers)
  ```





**人人网登录**

```python
import requests

post_url = "http://www.renren.com/PLogin.do"
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.79 Safari/537.36',
}
post_data = {
    'email': '18770915328',
    'password': '123456'
}

# 实例化Session对象
session = requests.Session()

# 使用Session对象发送登录post请求
resp = session.post(post_url, data=post_data, headers=headers)   #cookie会存在session中
print(resp)

print('--------------------------------')

# 使用Session对象发起get请求, 获取个人主页
profile_url = "http://www.renren.com/967971299/profile"
resp = session.get(profile_url, headers=headers)				#会带上之前的cookie
with open('renren.html', 'w', encoding="utf-8") as f:
    f.write(resp.content.decode())
```



### >2 cookie字符串放在headers中

- headers中的cookie：
  - 使用分号 (;) 隔开
  - 分号两边的类似a=b形式的表示一条cookie
  - 在headers中仅仅使用了cookie的name和value,  所以在代码中仅仅需要cookie的name和value即可
- cookie的具体组成的字段:  可以检查元素在Application项点击Cookies查看
- cookie有过期时间，直接复制浏览器中的cookie可能意味着下一程序继续运行的时候需要替换代码中的cookie，对应的我们也可以通过一个程序专门来获取cookie供其他程序使用；
- 当然也有很多网站的cookie过期时间很长，这种情况下，直接复制cookie来使用更加简单




**人人网登录**

```python
import requests

# url
url = "http://www.renren.com/967971299/profile"
headers = {
    "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1",
    "Cookie": ".....从浏览器请求头复制的cookie字符串......."
}
# 发起请求, 接受响应
resp = requests.get(url, headers=headers)
html = resp.content.decode()
with open('renren_2.html', 'w') as f:
    f.write(html)
```



### >3 请求时使用cookies字典

- cookies的形式：字典,  `cookies = {"cookie的name":"cookie的value"}`


- 使用方法：`requests.get(url,headers=headers,cookies=cookie_dict}`
- cookie字符串的分割split(sep=None, maxsplit=-1))
- 字典推导式



**人人网登录**

```python
import requests

# url
url = "http://www.renren.com/967971299/profile"
headers = {
    "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1",
}

# 等待转为字典的cookie字符串
cookies_str = ".....从浏览器请求头复制的cookie字符串......."

# ----------------------------------------------------------------
# cookies = {}
# li_equal = cookies_str.split("; ")   # 按  分号;+一个空格  分隔
# for eq in li_equal:
    
    # NO.1
    # ret = eq.split('=')
    # cookies[ret[0].strip()] = ret[1].strip()

    # NO.2
    # name, value = eq.split('=', maxsplit=1)   # todo 指明参数maxsplit=1, 只按字符串的第一个=号进行分隔
    # cookies[name.strip()] = value.strip()
# ----------------------------------------------------------------  

# NO.3		todo 字典推导式
cookies = {i.split('=', maxsplit=1)[0]:i.split('=', maxsplit=1)[1] for i in cookies_str.split('; ')}

print(cookies)
# 发起请求, 接收响应
resp = requests.get(url, headers=headers, cookies=cookies)
with open('renren_3.html', 'w', encoding="utf-8") as f:
    f.write(resp.content.decode())
```



1 2 3

1 方法先请求登录, 使用Session的实例对象再次请求时自动带上cookie

2 3 方法直接使用cookies,  cookies过期后就无法使用



### js分析流程

**如果action对应的有url地址**

- 查找登陆接口url:   登录的form表单中action对应的url地址
- 登录提交的参数key:  表单控件name属性对应的值
- 登录提交的value:  用户写入表单的值

**如果action对应的没有url地址**

- 通过NetWork抓包寻找登录的url地址 (NetWork 分析 FormData)




**分析js，获取加密的数据**

- 观察变化
- 定位js
  - 通过event listener定位js的位置
  - 通过搜索url地址中的关键字，通过chrome的search all file来进行搜索
- 进行分析
  - 通过添加断点的方式分析js
- 执行js
  - 完全的使用python模拟js的执行过程



**> js分析之查看js的位置**

**观察按钮的绑定js事件**

- 点击按钮提交到哪个url:  输入一个`错误的密码`, 点击提交, 抓包寻找登录的请求信息
- 刷新页面时 / 页面跳转时,  通过NetWork抓包默认会覆盖之前的响应信息,  可以勾选NetWork下的 `preserve log`  保留前面的信息.    可以捕捉 `登录时跳转页面` 的响应信息

![preserve_log](.\01_爬虫入门_images\preserve_log.png)



**通过搜索**:   搜索菜单 / Ctrl + F



**> js分析之观察js的执行过程**

找到js的位置之后，我们可以来通过观察js的位置，找到js具体在如何执行，后续我们可以`通过python程序来模拟js的执行`，或者是使用类似`js2py`直接把js代码转化为python程序去执行

观察js的执行过程最简单的方式是 `添加断点`

添加断点的方式：点击左边行号，对应的右边BreakPoints中会出现现有的所有断点

添加断点之后继续点击登录，每次程序在断点位置都会停止，通过如果该行有变量产生，都会把变量的结果展示在Scoope中



![格式化js](.\01_爬虫入门_images\格式化js.png)

![设置断点观察js代码的执行](.\01_爬虫入门_images\设置断点观察js代码的执行.png)



**> 执行js**

js也是一门动态的语言，所以在运行的时候需要js的解释器，目前现有的类似`js2py`的工具都不是特别完善，对于简单的js可以运行，但是处理不了复杂的js

很多js都是进过混淆和加密的，即在js的源码中我们会发现大量的命名为a,b,c,d的函数和变量，对于这种情况，需要通过程序去控制浏览器的方式操作网页、获取数据，这个方法会更加简单。





## 2.4 requests模块的其他方法

- requests模块获取cookie
- requests模块ssl证书错误的处理方法
- 超时参数
- retrying模块

### 获取cookie

`requests.utils.dict_from_cookiejar`:   把cookiejar对象转化为字典

```python
import requests

url = "http://www.baidu.com"
response = requests.get(url)
print(type(response.cookies))

cookies = requests.utils.dict_from_cookiejar(response.cookies)
print(cookies)
```

输出为:

```python
<class 'requests.cookies.RequestsCookieJar'>
{'BDORZ': '27315'}
```

在前面的requests的session类中，我们不需要处理cookie的任何细节，如果有需要，我们可以使用上述方法来解决



### 处理SSL认证

ssl的证书不安全可能导致处理证书错误,  在代码中请求时如果遇到ssl证书错误会 `抛出异常`

```python
import requests

url = "https://www.12306.cn/mormhweb/"   # todo  以https协议访问 可能会提示SSL证书错误
response = requests.get(url)
```

返回

```
ssl.CertificateError ...
```

![SSL证书错误](.\01_爬虫入门_images\SSL证书错误.png)



**解决方法:**     请求时指定 `verify=False`

```python
import requests

url = "https://www.12306.cn/mormhweb/"
response = requests.get(url,verify=False)
```



### 超时参数

在爬虫中，一个请求很久没有结果，就会让整个项目的效率变得非常低，这个时候我们就需要对请求进行强制要求，让它必须在特定的时间内返回结果，否则就报错.

**使用方法**：  请求时指定 timeout=xx (单位: 秒)

`response = requests.get(url,timeout=3)`

通过添加timeout参数，能够保证在3秒钟内返回响应，否则会报错

这个方法还能够拿来 `检测代理ip的质量`，如果一个代理ip在很长时间没有响应，那么添加超时之后也会报错，对应的这个ip就可以从代理ip池中删除



### retrying模块刷新请求

上述方法能够加快我们整体的请求速度，但是在正常的网页浏览过成功，如果发生速度很慢的情况，我们会做的选择是**刷新页面**，那么在代码中，我们是否也可以刷新请求呢？

对应的，retrying模块就可以帮助我们刷新请求

- retrying模块的地址：[https://pypi.org/project/retrying/](https://pypi.org/project/retrying/)
- retrying 模块的使用
  - 使用retrying模块提供的retry模块
  - 通过装饰器的方式使用，让被装饰的函数反复执行
  - retry中可以传入参数`stop_max_attempt_number`, 让函数报错后继续重新执行，达到最大执行次数的上限，如果每次都报错，整个函数报错，如果中间有一个成功，程序继续往后执行



**所以我们可以结合前面的知识点和retrying模块，把我们需要反复使用的请求方法做一个简单的封装，在后续任何其他地方需要使用的时候，调用该方法就行**



代码参考

```python

# parse.py

import requests
from retrying import retry


headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.79 Safari/537.36",
}
url = "http://www.12306.cn/mormhweb/"


@retry(stop_max_attempt_number=3)   # 最大重试3次，3次全部报错时，才会报错
def _parse_url():
    print('-------flag1------')   # flag1最多打印3次
    resp = requests.get(url, headers=headers, timeout=3, verify=False)  # 超时的时候会报错并重试
    assert resp.status_code == 200
    print('-------flag2------')   # flag2最多打印1次
    
    return resp.content.decode()


def parse_url():
    try:
        html = _parse_url()
    except Exception as e:
        print(e)
        html = None
    return html


if __name__ == '__main__':
    html = parse_url()
    if html:
        print(html[:50])
    else:
        print("请求无响应!")
```





## 2.5 chrome浏览器在爬虫中的使用

**新建隐身窗口(无痕窗口)**

在打开隐身窗口的时候，第一次请求某个网站是没有携带cookie的，和代码请求一个网站一样，不携带cookie。这样就能够尽可能的理解代码请求某个网站的结果；

除非数据是通过js加载出来的，不然爬虫请求到的数据和浏览器请求的数据大部分时候都是相同的



**chrome中network的更多功能**

**> Perserve log**

默认情况下，页面发生跳转之后，之前的请求url地址等信息都会消失，勾选perserve log后之前的请求都会被保留 

**> filter过滤**

过滤url地址

**> 观察特定种类的请求**

默认是选择的`all`，即会观察到所有种类的请求

- XHR:  大部分情况表示ajax请求
- JS:  js请求
- CSS:  css请求

ctrl + 鼠标左键:  多选

但是很多时候我们并不能保证我们需要的请求是什么类型，特别是我们不清楚一个请求是否为ajax请求的时候，直接选择`all`,  从前往后观察即可，其中js，css，图片等不去观察即可

**> 其他方法**

- search
- 确定 js 的位置
- js中添加断点



##   

# 3 数据提取方法

- 数据提取的基础概念和数据分类
- json模块
- 正则表达式
- xml
- xpath
- LXML类库
- 多线程和多进程爬虫

数据提取:  从响应中获取我们想要的数据的过程



## 3.1 爬虫中数据的分类

- 结构化数据：json，xml等
  - 处理方式：直接转化为python类型
- 非结构化数据：HTML
  - 处理方式：正则表达式、xpath

## 3.2 json

### 什么是json

JSON (JavaScript Object Notation) 是一种轻量级的数据交换格式，人很容易阅读和编写,  机器容易解析和生成。

适用于进行数据交互的场景，比如网站前台与后台之间的数据交互。



### 哪里能找到返回json的url

以豆瓣热映电影为例，https://movie.douban.com/cinema/nowplaying/beijing/ :

- PC端浏览器的豆瓣电影数据直接在 `html` 中, 用 正则/xpath/... 提取html字符串



- 移动端浏览器的豆瓣电影数据在返回的` JSON字符串` 中,  需要模拟ajax请求, 带上请求头参数User-Agent和Referer,  从返回的JSON字符串中提取有效数据

- 以上非通用,  具体情况具体分析

  **请求URL的分析:** 

  ```python
  # ------------原始请求url-------------
  # url = "https://m.douban.com/rexxar/api/v2/subject_collection/movie_showing/items?os=ios&for_mobile=1&callback=jsonp1&start=0&count=18&loc_id=108288&_=1536672027827"

  # ------------删除不必要参数-------------
  url = "https://m.douban.com/rexxar/api/v2/subject_collection/movie_showing/items?&loc_id=108288"

  注意点: 
      - 主要删除 callback 参数, 使返回的数据是正确的json字符串
  ```


**如何确定数据在哪里**

在url地址对应的响应中搜索关键字即可

但是注意：url地址对应的响应中，中文往往是被编码之后的内容，所以最好搜索英文和数字；另外一个方法就是在`perview`中搜索，其中的内容都是转码之后的

**切换手机版寻找返回json的地址**

在chrome中点击切换手机版的选项，需要重新刷新页面才能够切换成功，

部分网站还需要重新进入主页面之后再继续点击才能够切换成功，比如：豆瓣热映

现在我们找到了返回电影数据的地址：`https://m.douban.com/rexxar/api/v2/subject_collection/movie_showing/items?os=android&for_mobile=1&callback=jsonp1&start=0&count=18&loc_id=108288&_=1524495777522`

通过请求我们发现，并不能成功，那是为什么呢？

对比抓包的地址和现在的地址，会发现部分headers字段没有，通过尝试，会发现是`Referer`字段缺少的原因

在上面这个地址中，我们会发现响应中包含部分数据并不是我们想要的，如下：

```
;jsonp1({"count": 18, "start": 0, "subject_collection_items": "...."})
```

其中`jsonp1`似乎是很眼熟，在请求的地址中包含一个参数`callback=jsonp1`，正是由于这个参数的存在，才导致结果中也会有这部分数据，对应的解决方法是：直接删除url地址中的`callback`字段即可,在url地址中很多字段都是没用的，比如这里的`&loc_id`,`_`等等，可以大胆尝试

**json数据格式化的方法**

- 在线解析工具进行解析
  - 如：[https://www.bejson.com/](https://www.bejson.com/)
- pycharm进行 Reformat code
  - 在pycharm中新建一个json文件，把数据存入后，点击code下面的Reformat code，但是中文往往显示的是unicode格式



### json模块的使用

![JSON模块的使用](.\01_爬虫入门_images\JSON模块的使用.png)

**类文件对象**

> 具有read()或者write()方法的对象就是类文件对象，比如f = open(“a.txt”,”r”)  的 f 就是类文件对象



**json的方法：**

- json.dumps(json_dict) 	实现python类型转化为json字符串
   - json.dump(obj, fp) 	 实现把python类型写入类文件对象
- json.loads(json_str)  实现json字符串转化为python类型
     - json.load(fp)	 实现类文件对象中的json字符串转化为python类型



**方法的可选参数:**  

- indent=2   实现换行和空格
- ensure_ascii=False   解决中文乱码问题



```python
json_str = json.dumps(mydict,indent=2,ensure_ascii=False)

#json.loads 实现json字符串转化为python类型
my_dict = json.loads(json_str)

#json.dump 实现把python类型写入类文件对象
with open("temp.txt","w") as f:
    json.dump(mydict,f,ensure_ascii=False,indent=2)

# json.load 实现类文件对象中的json字符串转化为python类型
with open("temp.txt","r") as f:
    my_dict = json.load(f)
```



**JSON和python对应**

Json在数据交换中起到了一个载体的作用，承载相互传递的数据.

![json和python的对应](.\01_爬虫入门_images\json和python的对应.png)



## 3.3 正则表达式

### 什么是正则表达式

- Regular Expression --- regex、regexp或RE
- 用事先定义好的一些特定字符、及这些特定字符的组合，组成一个**规则字符串**，这个**规则字符串**用来表达对字符串的一种**过滤**逻辑。



### 正则表达式的常见语法

**1  匹配单个字符**

| **字符** | **功能**                              |
| ------ | ----------------------------------- |
| .      | 匹配任意1个字符（**除了\n**）  ---  \[^\n]     |
| [ ]    | 匹配**1个**[ ]中列举的字符                   |
| \d     | 匹配数字，即0-9  ---  [0-9]               |
| \D     | 匹配非数字，即不是数字  ---  \[^0-9]，**必须占一位** |
| \s     | 匹配空白字符，即 空格，tab键 --- [\t\n\r\r\v]   |
| \S     | 匹配非空白字符  --- **必须占一位**              |
| \w     | 匹配单词字符，即a-z、A-Z、0-9、_ 、**一个中文字**    |
| \W     | 匹配非单词字符   --- **必须占一位**             |

**空白字符**：空格、Tab制表符、\n、\t、**\r **(不换行的前提下将光标移到行首)、/v



**2  匹配多个字符**

| 字符    | 功能                                       |
| ----- | ---------------------------------------- |
| *     | 匹配**前一个字符**出现0次或者无限次，即可有可无  ---> {0,}    |
| +     | 匹配前一个字符出现1次或者无限次，即至少有1次  ---> {1,}       |
| ?     | 匹配前一个字符出现1次或者0次，即要么有1次，要么没有 ---> {0, 1}  |
| {m}   | 匹配前一个字符出现m次                              |
| {m,}  | 匹配前一个字符至少出现m次                            |
| {,m}  | 匹配前一个字符最多出现m次，可以不出现                      |
| {m,n} | 匹配前一个字符出现从m到n次 （**m,n之间不能有空格**）**正则里避免用空格** |



**3  匹配开头结尾**

| 字符   | 功能        |
| ---- | --------- |
| ^    | 匹配字符串开头   |
| $    | 匹配字符串结尾   |
| \b   | 匹配一个单词的边界 |
| \B   | 匹配非单词边界   |

```python
----------------- “ ^ ” 匹配开头的情况 ----------------- 
/[(^\s+)(\s+$)]/g
(^cat)$(^cat$)
^(cat)$^(cat$)

----------------- “ ^ ” 匹配取反的情况 ----------------- 
当 “ ^ ” 出现在一个字符集合模式的第一个字符时，表示字符类的否定
[^a] 			表示“匹配除了a的任意字符”
[^a-zA-Z0-9] 	表示“找到一个非字母也非数字的字符”
[\^abc] 		表示“找到一个插入符或者a或者b或者c”
[^\^] 			表示“找到除了插入符外的任意字符”

”[]”代表的是一个字符集，”^”只有在[字符集]中才是反向字符集的意思


-------- 匹配单词边界 \b --------
匹配到单词is：
>>> re.search(r'\bis\b','this island is beautiful').span()  
>>> (12, 14)
```



**4  匹配分组**

| 字符         | 功能                             |
| ---------- | ------------------------------ |
| \|         | 匹配左右任意一个表达式                    |
| (ab)       | 将括号中字符作为一个分组                   |
| \num       | 引用分组num匹配到的字符串，默认引用从 **\1** 开始 |
| (?P<name>) | 分组起别名                          |
| (?P=name)  | 引用别名为name分组匹配到的字符串             |





### re模块的常用方法

- re.match()		从头找一个
   - re.search()找一个
- **re.findall()找所有**,  返回一个列表，没有就是空列表
   - re.sub()替换,  返回一个列表
   - re.split()根据匹配进行切割字符串，并返回一个列表
- re.span()返回pattern在str中的索引值组成的元组   re.search(r'is','this island is beautiful').span()--->**(2, 4)**


- **re.compile()		编译**

  - 返回一个模型P ( `Pattern实例` )，具有和re一样的方法，但是传递的参数不同

  - 匹配模式需要传到compile中

  - 经过re.compile()编译后再匹配,  匹配效率高于re本身的方法

    ```
    p = re.compile("\d",re.S)
    p.findall("chuan1zhi2")
    ```

**re方法的使用**

```python
>>> re.search(r"123|456", "456abc123").group()
>>> '456'   # 在string中找到第一个pattern匹配的字符串就返回这个字符串


>>> re.match(r"(123|456)[a-z]{3}", "456abc123").group()
>>> '456abc'    # 如果string中包含pattern，调用.group()方法直接返回pattern字符串

>>> re.findall(r"(123|456)[a-z]{3}", "456abc123")
>>> ['456']     #  如果string中包含有pattern分组中的内容，返回分组中对应字符串组成的列表

>>> re.sub(r"(123|456)[a-z]{3}", "IIII", "456abc123")
>>> 'IIII123'    #　如果在string中找到pattern匹配的内容，返回在string基础上进行修改后的字符串


>>> re.split(r"[a-z]", "456abc123")
>>> ['456', '', '', '123']
----匹配到'a', "456", "bc123"----匹配到'b', "456", "", "c123"----匹配到'c', ["456", "", "", "123"]

>>> re.split(r"(123|456)", "456abc123")
>>> ['', '456', 'abc', '123', '']  #  根据string中pattern匹配的内容分割string，如果pattern在分组中，保留pattern
>>> re.split(r"123|456", "456abc123")
>>> ['', 'abc', '']    #  根据string中pattern匹配的内容分割string，如果pattern不在分组中，不保留pattern，结果是pattern变成空字符串 ””
```



### python贪婪和非贪婪

- 非贪婪：条件成立的情况下 取最少
- 贪婪：条件成立的情况下 取最多
- 在 "*",  "?",  "+",  "{m,n}" 后面加上？，使贪婪变成非贪婪
   - **.*?非贪婪模式匹配所有 除了换行\n**




**> 贪婪.* 和非贪婪.*? 注意点**

- 字符串中间	尽量使用非贪婪匹配
- 字符串末尾尽量使用贪婪匹配

```python
【1】
import re 
content = 'Hello 1234567 World_This is a Regex Demo' 
result = re.match('^He.*(\d+).*Demo$', content) 
print(result.group(1))
运行结果：7

奇怪的事情发生了，我们只得到了7这个数字，这是怎么回事呢？
这里就涉及一个贪婪匹配与非贪婪匹配的问题了。在贪婪匹配下，.*会匹配尽可能多的字符。正则表达式中.*后面是\d+，也就是至少一个数字，并没有指定具体多少个数字，因此，.*就尽可能匹配多的字符，这里就把123456匹配了，给\d+留下一个可满足条件的数字7，最后得到的内容就只有数字7了。

将第一个.*改成了.*?，转变为非贪婪匹配
在做匹配的时候，字符串中间尽量使用非贪婪匹配，也就是用.*?来代替.*，以免出现匹配结果缺失的情况。


【2】
但这里需要注意，如果匹配的结果在字符串结尾，.*?就有可能匹配不到任何内容了，因为它会匹配尽可能少的字符。例如：

import re 
content = 'http://weibo.com/comment/kEraCN' 
result1 = re.match('http.*?comment/(.*?)', content) 
result2 = re.match('http.*?comment/(.*)', content) 
print('result1', result1.group(1)) 
print('result2', result2.group(1))

运行结果：
result1 
result2 kEraCN
```





### 原始字符串 r

**原始字符串定义(raw string)**：所有的字符串都是直接按照字面的意思来使用，没有转义特殊或不能打印的字

原始字符串往往针对 `特殊字符` 而言。例如`"\n"`的原始字符串就是`"\\n"`

- 相对于 `特殊符号` 而言，表示 `特殊符号` 的字面意思
- Python中字符串前面加上 r 表示原生字符串，不用转义
- 用途
  - 正则：忽略转义符号带来的影响，加上r之后照着写  `\`
  - windows下文件路径

- 原始字符串的长度

  ```python
  >>> len("\n")
  >>> 1

  >>> len(r"\n")
  >>> 2

  >>> r"\n"[0]
  >>> '\\'
  ```

- 正则中原始字符串的使用

  ```python
  >>> r"a\nb" == "a\\nb"
  >>> True

  >>> re.findall("a\nb","a\nb")
  >>> ['a\nb']

  >>> re.findall(r"a\nb","a\nb")		# 加 r 的好处:  忽略特殊字符串的转义
  >>> ['a\nb']

  >>> re.findall("a\\nb","a\nb")
  >>> ['a\nb']

  >>> re.findall("a\\nb","a\\nb")
  >>> []

  >>> re.findall(r"a\\nb","a\\nb")	# 加 r 的好处:  忽略特殊字符串的转义
  >>> ['a\\nb']
  ```

  上面的现象说明什么？

> 正则中使用原始字符串`r`能够忽略转义符号带来的影响，加上原始字符串`r`之后，待匹配的字符串中有多少个`\`，正则中就添加多少个`\`即可
> 假如需要匹配文本中的字符"\"，那么使用编程语言表示的正则表达式里将需要4个反斜杠"\\"：
> 前两个和后两个分别用于在编程语言里转义成反斜杠，转换成两个反斜杠后再在正则表达式里转义成一个反斜杠。

```python
如果在正则表达式中需要用到匹配规则字符的原始类型，如 .  ? 等，就要在前面加上 反斜杠 \
特殊字符串				原始字符串
\.			---->			.
\\n							\\n
\\t							\\t
\?
\\r
\*
```



**> 转义匹配**

当遇到**用于正则匹配模式的特殊字符**时，在前面加反斜线转义一下即可。例如 `.` 就可以用`\.`来匹配

 ```python
import re 

content = '(百度)www.baidu.com' 
result = re.match('\(百度\)www\.baidu\.com', content) 
print(result)
 ```



### 修饰符控制匹配模式

正则表达式可以包含一些可选标志修饰符来控制匹配的模式。修饰符被指定为一个可选的标志。

| **修饰符**  | **描述**                                  |
| -------- | --------------------------------------- |
| **re.I** | 使匹配对大小写不敏感    ( re.IGNORECASE 的简写)      |
| re.L     | 做本地化识别（locale-aware）匹配                  |
| re.M     | 多行匹配，影响^和$                              |
| **re.S** | 使 点. 匹配包括换行在内的所有字符,   ( re.DOTALL 的简写 ) |
| re.U     | 根据Unicode字符集解析字符。这个标志影响\w、\W、 \b和\B     |
| re.X     | 该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解          |

在网页匹配中，较为常用的有 `re.S` 和 `re.I`



练习: 

通过正则匹配果壳问答上面的精彩回答的地址和标题:[https://www.guokr.com/ask/highlight/?page=1](https://www.guokr.com/ask/highlight/?page=1) 思路：

1. 寻找url地址的规律
2. 寻找数据的位置

获取36kr上的所有新闻：[http://36kr.com/](http://36kr.com/)

>  爬取果壳网精彩回答

```python
# coding=utf-8
import requests
import re


class Gouke:
    def __init__(self):
        self.url_temp = "https://www.guokr.com/ask/highlight/?page={}"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Linux; Android 5.0; SM-G900P Build/LRX21T) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.117 Mobile Safari/537.36"
        }

    def get_url_list(self):
        return [self.url_temp.format(i) for i in range(1, 101)]

    def parse_url(self, url):
        print(url)
        resp = requests.get(url, headers=self.headers)
        html_str = resp.content.decode()
        return html_str

    def get_content_list(self, html_str):
        content_list = re.findall(r"<h2><a target=\"_blank\" href=\"(.*?)\">(.*?)</a></h2>", html_str, re.S)
        return content_list

    def save_content_list(self, content_list):  # 提取数据
        for content in content_list:
            print(content)

    def run(self):  # 实现主要逻辑
        # 1. url_list
        url_list = self.get_url_list()
        # 2. 遍历，发送请求，获取响应
        for url in url_list:
            html_str = self.parse_url(url)
            # 3. 提取数据
            content_list = self.get_content_list(html_str)
            # 4.保存
            self.save_content_list(content_list)


if __name__ == '__main__':
    guoke = Gouke()
    guoke.run()
```





## 3.4 xpath语言和 lxml 解析器

XML Path Language	最初是用来搜寻XML文档的，但是同样适用于HTML文档的搜索。

XPath是一门在 HTML\XML 文档中查找信息的**语言**，可用来在  **HTML\XML** 文档中对**元素和属性进行遍历**。

W3School官方文档：[http://www.w3school.com.cn/xpath/index.asp](http://www.w3school.com.cn/xpath/index.asp)

lxml是一款高性能的 Python HTML/XML 解析器，我们可以利用XPath，来快速的定位特定元素以及获取节点信息



### html和xml语言

XML:  ExtentsibleMarkup Language(可扩展标记语言),  是一种跨平台的，与软、硬件无关的，处理与传输信息的工具.

在JSON之前,  是使用XML进行数据交互.



**> html和xml的区别**

​    xml和html都是用于操作数据或数据结构，在结构上大致是相同的，但它们在本质上却存在着明显的区别。

综合网上的各种资料总结如下:

（一）、语法要求不同：

|      | HTML        | XML            |
| ---- | ----------- | -------------- |
| 大小写  | 不区分大小写      | 严格区分大小写        |
| 结束标记 | 有时可以省略结束标记  | 绝对不能省略掉结束标记    |
| 单个标记 | 不必加/ 字符作为结尾 | 必须用一个/ 字符作为结尾  |
| 属性值  | 引号可用可不用     | 属性值必须在引号中      |
| 属性名  | 可以拥有不带值的属性名 | 所有的属性都必须带有相应的值 |
| 空白部分 | html会过滤掉空格  | 空白部分不会被解析器自动删除 |

（二）、标记不同：

|      | HTML        | XML                 |
| ---- | ----------- | ------------------- |
| 标记   | 使用固有的标记     | 没有固有的标记             |
| 标签   | Html标签是预定义的 | XML标签是免费的、自定义的、可扩展的 |

（三）、作用不同：

|      | HTML         | XML                         |
| ---- | ------------ | --------------------------- |
| 作用   | html用来显示数据   | XML用来描述数据、存放数据,  可以作为持久化的介质 |
| 设计目标 | 显示数据并集中于数据外观 | 描述数据并集中于数据的内容               |
| 行为   | 没有任何行为       | 没有任何行为                      |



**> xml的树结构**

```xml
<bookstore>
<book category="COOKING">
  <title lang="en">Everyday Italian</title> 
  <author>Giada De Laurentiis</author> 
  <year>2005</year> 
  <price>30.00</price> 
</book>
<book category="CHILDREN">
  <title lang="en">Harry Potter</title> 
  <author>J K. Rowling</author> 
  <year>2005</year> 
  <price>29.99</price> 
</book>
<book category="WEB">
  <title lang="en">Learning XML</title> 
  <author>Erik T. Ray</author> 
  <year>2003</year> 
  <price>39.95</price> 
</book>
</bookstore>
```

![xml树结构](.\01_爬虫入门_images\xml树结构.jpg)

### xpath的节点关系

节点:  每个XML的标签
根节点:  最顶层的节点
![xpath中节点的关系](.\01_爬虫入门_images\xpath中节点的关系.png)



### **xpath中节点选择的插件**

- Chrome插件 **XPath Helper**
  - 下载地址：https://pan.baidu.com/s/1UM94dcwgus4SgECuoJ-Jcg  密码:337b
- Firefox插件 **XPath Checker**
- 注意： 这些工具是用来**学习xpath语法**的，他们都是**从elements中**匹配数据，elements中的数据和url地址对应的响应不相同，所以在代码中，不建议按照这些工具的xpath路径进行数据的提取



### xpath语法

**> 选取节点**

XPath 使用路径表达式来选取 XML 文档中的节点或者节点集。这些路径表达式和我们在常规的**电脑文件系统中看到的表达式**非常相似。

**使用chrome插件选择标签时，选中时，选中的标签会添加属性class="xh-highlight", 该类名不能写进xpath 路径**



**最有用的表达式**：

| 表达式        | 描述                                    |
| ---------- | ------------------------------------- |
| nodename   | 选中该元素,  如 div标签元素 / a标签元素             |
| **/**      | 从根节点选取、或者是元素和元素间的过渡                   |
| **//**     | 从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置          |
| **.**      | 选取当前节点     ./    .//                  |
| **..**     | 选取当前节点的父节点     ../    ..//            |
| **@**      | 选取属性   /@href    /@src    /@title.... |
| **text()** | 选取文本   /text()     //text()           |



>  实例:

| 路径表达式               | 结果                                       |
| ------------------- | ---------------------------------------- |
| bookstore           | 选择bookstore元素。                           |
| /bookstore          | 选取根元素 bookstore。注释：假如路径起始于正斜杠( / )，则此路径始终代表到某元素的绝对路径！ |
| bookstore/book      | 选取属于 bookstore 的子元素的所有 book 元素。          |
| //book              | 选取所有 book 子元素，而不管它们在文档中的位置。              |
| bookstore//book     | 选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置。 |
| //book/title/@lang  | 选择所有的book下面的title中的lang属性的值。             |
| //book/title/text() | 选择所有的book下面的title的文本。                    |



>  练习：

接下来我们听过豆瓣电影top250的页面来练习上述语法：[https://movie.douban.com/top250](https://movie.douban.com/top250)

- 选择所有的h1下的文本					`//h1/text()`
   - 获取所有的a标签的href		`//a/@href`
- 获取html下的head下的title的文本`/html/head/title/text()`
- 获取html下的head下的link标签的href`/html/head/link/@href`

但是当我们需要选择所有的电影名称的时候会特别费力，后面能够解决这个问题



**>  查找特定的节点**

| 过滤条件     | 路径表达式                                   | 结果                                       |
| :------- | :-------------------------------------- | ---------------------------------------- |
| 限制元素的属性值 | **//title[@lang="eng"]**                | 选择lang属性值为eng的所有title元素                  |
| 哪个子元素    | **/bookstore/book[1]**                  | 选取属于 bookstore 子元素的第一个 book 元素。**(!! 从1开始)** |
| 哪个子元素    | /bookstore/book[last()]                 | 选取属于 bookstore 子元素的最后一个 book 元素。         |
| 哪个子元素    | **/bookstore/book[last()-1]**           | 选取属于 bookstore 子元素的倒数第二个 book 元素。        |
| 哪些子元素    | **/bookstore/book[position()>1]**       | 选择bookstore下面的book元素，从第二个开始选择            |
| 文本限制     | **//book/title[text()='Harry Potter']** | 选择所有book下的title元素，仅仅选择文本为Harry Potter的title元素 |
| 文本包含     | **//a[contains(text(), "下一页")]/@href**  | 选择所有a标签下的文本包含"下一页"的 href属性值              |
| 文本相等     | **//a[text()="下一页>"]/@href**            | 选择所有a标签下的文本是  "下一页>"  的元素 的href属性值       |
| 属性值包含    | **//a[contains(@class, "xx")]**         | 选择所有 class属性的值包含"xx"的元素                  |
| 元素值限制    | /bookstore/book[price>35.00]/title      | 选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。 |
|          |                                         |                                          |

注意点: **在xpath中，第一个元素的位置是1，最后一个元素的位置是last(),倒数第二个是last()-1**



练习：选择所有的电影的名称，href，评分，评价人数

- 所有电影名称:   只含中文: 	`//span[@class="title"][1]/text()`	
   - 所有电影href:	`//div[@class="hd"]/a/@href`
    - 所有电影评分: `//span[@class='rating_num']/text()`
- 所有电影评价人数: `//div[@class="star"]/span[last()]/text()`
- 获取底部分页显示的所有文本, 包括数字1-10:`//div[@class="paginator"]//text()`



**> 选取未知节点**

XPath 通配符选取未知节点

| 通配符    | 描述         |
| ------ | ---------- |
| *      | 匹配任何元素节点。  |
| @*     | 匹配任何属性节点。  |
| node() | 匹配任何类型的节点。 |



> 实例

| 路径表达式        | 结果                     |
| ------------ | ---------------------- |
| /bookstore/* | 选取 bookstore 元素的所有子元素。 |
| //*          | 选取文档中的所有元素。            |
| //title[@*]  | 选取所有带有属性的 title 元素。    |



还是上面的练习：选择所有的电影的名称，href，评分，评价人数

> 只需要将[@class="xxx"]之前的标签名改为 * 号 或 node(),  其他都不变



**> 选取若干路径**

通在路径表达式中使用 “|” 运算符可以选取若干个路径。

路径表达式1 | 路径表达式2



> 实例

| 路径表达式                            | 结果                                       |
| -------------------------------- | ---------------------------------------- |
| //book/title \| //book/price     | 选取 book 元素的所有 title 和 price 元素。          |
| //title \| //price               | 选取文档中的所有 title 和 price 元素。               |
| /bookstore/book/title \| //price | 选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素。 |



在xpath 的语法中使用最多的是    `//`,    `@`,    `text()` 





## 3.5 lxml

- 在代码中使用 xpath 对数据处理和提取
- 安装lxml模块:   `pip3 install lxml`

![lxml模块的使用](.\01_爬虫入门_images\lxml模块的使用.png)



### lxml初级

1. 导入lxml 的 etree 库 ( 导入时没有提示,  不代表不能用 )

   ```python
   from lxml import etree
   ```

2. 利用etree.HTML()，将字符串/字节转化为Element对象,  Element对象具有xpath的方法,  返回结果的列表，能够接受bytes类型的数据和str类型的数据

   etree.HTML实例化时会对html进行处理,  把未封闭的标签进行封闭  (这个过程可能会导致标签`嵌套/并列`关系的错误)

   ```python
   element = etree.HTML(text)  # 实例化Element对象
   ret_list = element.xpath("xpath字符串")  # 获取Element对象列表 / 对象属性值
   ```

3. 把转化后的element对象转化为字符串，返回**bytes类型**结果 `etree.tostring(element)`

   **可以通过这个方法看一下 lxml 把html处理成什么样了,  需要根据处理后的html提取数据,  因为lxml处理结果不一定准确**

尝试对现有的html字符串进行操作

```python
from lxml import etree
text = ''' <div> <ul> 
        <li class="item-1"><a href="link1.html">first item</a></li> 
        <li class="item-1"><a href="link2.html">second item</a></li> 
        <li class="item-inactive"><a href="link3.html">third item</a></li> 
        <li class="item-1"><a href="link4.html">fourth item</a></li> 
        <li class="item-0"><a href="link5.html">fifth item</a> 
        </ul> </div> '''

html = etree.HTML(text)
print(type(html)) 

handeled_html_str = etree.tostring(html).decode()
print(handeled_html_str)
```

输出为

```html
<class 'lxml.etree._Element'>
<html><body><div> <ul> 
        <li class="item-1"><a href="link1.html">first item</a></li> 
        <li class="item-1"><a href="link2.html">second item</a></li> 
        <li class="item-inactive"><a href="link3.html">third item</a></li> 
        <li class="item-1"><a href="link4.html">fourth item</a></li> 
        <li class="item-0"><a href="link5.html">fifth item</a> 
        </li></ul> </div> </body></html>			<!-- 补充了缺失</li>, 补充了html和body标签 --> 
```

可以发现，lxml确实能够把缺失的标签补充完成，但是请注意**lxml是人写的，很多时候由于网页不够规范，或者是lxml的bug，即使参考url地址对应的响应去提取数据，仍然获取不到，这个时候我们需要使用etree.tostring的方法，观察etree到底把html转化成了什么样子，即根据转化后的html字符串去进行数据的提取**。



### lxml中级

接下来我们继续操作，假设每个class为item-1的li标签是1条新闻数据，如何把这条新闻数据组成一个字典

```python
from lxml import etree
text = ''' <div> <ul> 
        <li class="item-1"><a href="link1.html">first item</a></li> 
        <li class="item-1"><a href="link2.html">second item</a></li> 
        <li class="item-inactive"><a href="link3.html">third item</a></li> 
        <li class="item-1"><a href="link4.html">fourth item</a></li> 
        <li class="item-0"><a href="link5.html">fifth item</a> 
        </ul> </div> '''

html = etree.HTML(text)

# 获取href的列表和title的列表
href_list = html.xpath("//li[@class='item-1']/a/@href")
title_list = html.xpath("//li[@class='item-1']/a/text()")

# 组装成字典
for href in href_list:
    item = {}
    item["href"] = href
    item["title"] = title_list[href_list.index(href)]
    print(item)
```

输出为

```python
{'href': 'link1.html', 'title': 'first item'}
{'href': 'link2.html', 'title': 'second item'}
{'href': 'link4.html', 'title': 'fourth item'}
```

假设在某种情况下，某个新闻的href没有，那么会怎样呢？

```python
from lxml import etree
text = ''' <div> <ul> 
        <li class="item-1"><a>first item</a></li> 
        <li class="item-1"><a href="link2.html">second item</a></li> 
        <li class="item-inactive"><a href="link3.html">third item</a></li> 
        <li class="item-1"><a href="link4.html">fourth item</a></li> 
        <li class="item-0"><a href="link5.html">fifth item</a> 
        </ul> </div> '''
```

结果是

```python
{'href': 'link2.html', 'title': 'first item'}
{'href': 'link4.html', 'title': 'second item'}
```

数据的对应全部错了，这不是我们想要的，接下来解决这个问题



### lxml高级

**>>> 先分组,  再提取 <<<**

- 取到属性或者文本 的时候，返回字符串
- 但是如果取到的是**一个节点，返回的是element对象，可以继续使用xpath方法**，
  - 对此可以在后面的数据提取过程中：**先根据某个标签进行分组，分组之后再进行数据的提取**

示例如下：

```python
from lxml import etree
text = ''' <div> <ul> 
        <li class="item-1"><a>first item</a></li> 
        <li class="item-1"><a href="link2.html">second item</a></li> 
        <li class="item-inactive"><a href="link3.html">third item</a></li> 
        <li class="item-1"><a href="link4.html">fourth item</a></li> 
        <li class="item-0"><a href="link5.html">fifth item</a> 
        </ul> </div> '''

html = etree.HTML(text)

li_list = html.xpath("//li[@class='item-1']")
print(li_list)
```

结果为：

```python
[<Element li at 0x11106cb48>, <Element li at 0x11106cb88>, <Element li at 0x11106cbc8>]
```

可以发现结果是一个element对象，这个对象能够继续使用xpath方法

先根据li标签进行分组，之后再进行数据的提取

```python
from pprint import pprint
from lxml import etree

text = ''' <div> <ul> 
        <li class="item-1"><a href="link1.html">first item</a></li> 
        <li class="item-1"><a href="link2.html">second item</a></li> 
        <li class="item-inactive"><a>third item</a></li> 
        <li class="item-1"><a href="link4.html">fourth item</a></li> 
        <li class="item-0"><a href="link5.html">fifth item</a> 
        </ul> </div> '''

element_obj = etree.HTML(text)
fixed_html = etree.tostring(element_obj).decode()
print(fixed_html)

li_list = element_obj.xpath("//li")
print(li_list)

ret_list = list()
# 遍历element对象列表, 构造字典, 追加到列表末尾
for li in li_list:
    ret_dict = dict()
    ret_dict['href'] = li.xpath("./a/@href")[0] if li.xpath("./a/@href") else None
    ret_dict['text'] = li.xpath("./a/text()")[0] if li.xpath("./a/text()") else None
    ret_list.append(ret_dict)

pprint(ret_list)
```

结果是：

```python
{'href': None, 'title': 'first item'}
{'href': 'link2.html', 'title': 'second item'}
{'href': 'link4.html', 'title': 'fourth item'}
```

前面的代码中，进行数据提取需要判断，可能某些一面不存在数据的情况，对应的可以使用三元运算符来解决

以上提取数据的方式：先分组再提取，都会是我们进行数据的提取的主要方法



**百度贴吧数据爬取**

用XPath来做一个简单的爬虫，爬取某个贴吧里的所有帖子，获取每个帖子的<标题>，<连接>和帖子中<图片>

思路分析：

1. 推荐使用极速版的旧版手机页面，响应不包含js，elements和url地址对应的响应一样

2. **列表页**标题和链接

   - 确定url地址，确定程序停止的条件

    ​url地址的数量不固定，不能够去构造url列表，需要手动获取下一页的url地址进行翻页,  最后一页没有'下一页'按钮.

   - 确定列表页数据的位置

     由于没有js，可以直接从elements中进行数据的提取

3. **详情页**图片url

   - 确定url地址:  url详情页的规律和列表页相似, 也有分页


- 确定数据的位置



代码:

- 获取列表页的每一个帖子的 title,  url,  列表页有 "下一页"

- 获取每一个帖子对应详情页中的图片url,  详情页也有 "下一页"

- 使用 `递归函数` 请求详情页的下一页

- 获取到图片url对应的是低分辨率的图片,  需要先`解码`,  再提取其中高清图片的url:

  **------url解码------:**  	`requests.utils.unquote(i)`

```python
# coding=utf-8
import requests
from lxml import etree


class TieBaSpider:
    def __init__(self, tieba_name):
        # 1. start_url
        self.start_url = "http://tieba.baidu.com/mo/q---C9E0BC1BC80AA0A7CE472600CDE9E9E3%3AFG%3D1-sz%40320_240%2C-1-3-0--2--wapp_1525330549279_782/m?kw={}&lp=6024".format(
            tieba_name)
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Linux; Android 8.0; Pixel 2 Build/OPD3.170816.012) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Mobile Safari/537.36"}
        self.part_url = "http://tieba.baidu.com/mo/q---C9E0BC1BC80AA0A7CE472600CDE9E9E3%3AFG%3D1-sz%40320_240%2C-1-3-0--2--wapp_1525330549279_782"

    def parse_url(self, url):  # 发送请求，获取响应
        print(url)
        response = requests.get(url, headers=self.headers)
        return response.content

    def get_content_list(self, html_str):  # 3. 提取数据
        html = etree.HTML(html_str)
        div_list = html.xpath("//body/div/div[contains(@class,'i')]")
        content_list = []
        for div in div_list:
            item = {}
            item["href"] = self.part_url + div.xpath("./a/@href")[0]
            item["title"] = div.xpath("./a/text()")[0]
            item["img_list"] = self.get_img_list(item["href"], [])
            content_list.append(item)

        # 提取下一页的url地址
        next_url = html.xpath("//a[text()='下一页']/@href")
        next_url = self.part_url + next_url[0] if len(next_url) > 0 else None
        return content_list, next_url

    def get_img_list(self, detail_url, img_list):
        # 1. 发送请求，获取响应
        detail_html_str = self.parse_url(detail_url)
        # 2. 提取数据
        detail_html = etree.HTML(detail_html_str)
        img_list += detail_html.xpath("//img[@class='BDE_Image']/@src")

        # 详情页下一页的url地址
        next_url = detail_html.xpath("//a[text()='下一页']/@href")
        next_url = self.part_url + next_url[0] if len(next_url) > 0 else None
        if next_url is not None:  # 当存在详情页的下一页，请求
            
            # -------- 函数递归 --------
            return self.get_img_list(next_url, img_list)

        # -------- 递归条件不满足时 --------
        img_list = [requests.utils.unquote(i).split("src=")[-1] for i in img_list]
        return img_list

    def save_content_list(self, content_list):  # 保存数据
        for content in content_list:
            print(content)

    def run(self):  # 实现主要逻辑
        next_url = self.start_url
        while next_url is not None:
            # 1. start_url
            # 2. 发送请求，获取响应
            html_str = self.parse_url(next_url)
            # 3. 提取数据
            content_list, next_url = self.get_content_list(html_str)
            # 4。保存
            self.save_content_list(content_list)
            # 5.获取next_url，循环2-5


if __name__ == '__main__':
    tieba = TieBaSpider("python爬虫")
    tieba.run()
```





## 3.6 多线程和多进程爬虫

### 多线程

```python
# 创建线程对象
t1 = threading.Thread(target=func,args=(,))

# 把子线程设置为守护线程,即该线程不重要，主线程结束，子线程结束
t1.setDaemon(True)

# 启动子线程
t1.start()
```



### 队列模块

为什么使用队列?

- 控制主线程什么时候结束
- 数据读/写  **解耦**

```python
from queue import Queue
q = Queue(maxsize=100)
item = {}
q.put_nowait(item) 	# 不等待直接放，队列满的时候会报错
q.put(item) 		# 放入数据，队列满的时候会等待
q.get_nowait() 		# 不等待直接取，队列空的时候会报错
q.get() 			# 取出数据，队列为空的时候会等待
q.qsize() 			# 获取队列中现存数据的个数 
q.join() 			# 队列中维持了一个计数，计数不为0时候让主线程阻塞等待，队列计数为0的时候才会继续往后执行
q.task_done() 		# put的时候计数+1，get不会-1，get需要和task_done 一起使用才会-1
```



### 多线程爬虫思路

1. 把爬虫中的每个步骤封装成函数，分别用线程去执行
2. 不同的函数通过队列相互通信，函数间解耦 



![多线程爬虫](.\01_爬虫入门_images\多线程爬虫.png)



====================**具体思路**====================

1- 从url出发,  

​			<1> 让主线程开启多个子线程,  然后等待任务队列中的计数为0 再结束  (q.join())
​			<2> 一个子线程去生成 url队列  (for in)
​			<3> 一(多)个子线程去从url队列中取出url,  发起请求,  将响应数据存入响应队列  (while True)
​			<4> 一个子线程从响应队列中提取数据,  存入数据队列  (while True)
​			<5> 一个子线程去保存数据   (while True)

2- 子线程什么时候结束?		t.setDaemon(True)  把子线程设置为守护线程,  等待主线程结束后 结束

3- 主线程什么时候结束?		让所有队列对象q调用: q.join() 	即让主线程阻塞，等待队列中任务为空

4- 队列中任务什么时候为空 ?	队列计数为0时

5- 队列计数什么时候为0 ? 		q.put()时+1,  q.task_done()时-1,  (注意get()不会-1)

> 子线程等待主线程结束再结束,  主线程等待队列计数为0时结束



>  多线程爬取糗百

```python
# coding=utf-8
import requests
from lxml import etree
from queue import Queue
import threading
import time

class QiuBai:
    def __init__(self):
        self.temp_url = "http://www.qiushibaike.com/8hr/page/{}"
        self.headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36"}
        self.url_queue = Queue()
        self.html_queue = Queue()
        self.content_list_queue = Queue()

    def get_url_list(self):
        # return [self.temp_url.format(i) for i in range(1,14)]
        for i in range(1,14):
            self.url_queue.put(self.temp_url.format(i))

    def parse_url(self):
        while True:
            url = self.url_queue.get()
            response = requests.get(url,headers=self.headers)
            print(response)

            if response.status_code != 200:  # 如果未正常响应, 将url重新加入队列末尾, 之后继续请求
                self.url_queue.put(url)
            else:
                self.html_queue.put(response.content.decode())
            self.url_queue.task_done()  # 让队列的计数-1

    def get_content_list(self):	# 提取数据
        while True:
            html_str = self.html_queue.get()
            html = etree.HTML(html_str)
            div_list = html.xpath("//div[@id='content-left']/div")
            content_list = []
            for div in div_list:
                item = {}
                item["user_name"] = div.xpath(".//h2/text()")[0].strip()
                item["content"] = [i.strip() for i in div.xpath(".//div[@class='content']/span/text()")]
                content_list.append(item)
            self.content_list_queue.put(content_list)
            self.html_queue.task_done()

    def save_content_list(self): # 保存
        while True:
            content_list = self.content_list_queue.get()
            for content in content_list:
                # print(content)
                pass
            self.content_list_queue.task_done()

    def run(self):	# 实现做主要逻辑
        thread_list = []
        # 1. 一个子线程去准备url列表
        t_url = threading.Thread(target=self.get_url_list)
        thread_list.append(t_url)
        # 2. 多个子线程去发送请求，获取响应
        for i in range(3):
            t_parse = threading.Thread(target=self.parse_url)
            thread_list.append(t_parse)
        # 3. 一个子线程去提取数据
        t_content = threading.Thread(target=self.get_content_list)
        thread_list.append(t_content)
        # 4. 一个子线程去保存数据
        t_save = threading.Thread(target=self.save_content_list)
        thread_list.append(t_save)
		
        # 启动所有子线程
        for t in thread_list:
            t.setDaemon(True) 	# <1> 把子线程设置为守护线程
            t.start()
        for q in [self.url_queue,self.html_queue,self.content_list_queue]:
            q.join() 			# <2> 让主线程阻塞，等待队列计数为0


if __name__ == '__main__':
    t1 = time.time()
    qiubai = QiuBai()
    qiubai.run()
    print("total cost:",time.time()-t1)
```



### 多进程爬虫

```python
from multiprocessing import Process
t1 = Process(targe=func,args=(,))
t1.daemon = True  # 设置子进程为守护进程, 与多线程的写法不同
t1.start()  # 此时线程才会启动
```

### 多进程的队列模块 multiprocessing.JoinableQueue

`multiprocessing`提供的`JoinableQueue`模块

```python
from multiprocessing import JoinableQueue

...
self.url_queue = JoinableQueue()
self.html_queue = JoinableQueue()
self.content_list_queu = JoinableQueue()
```





## 3.7 线程池  协程池

通过线程池实现更快的爬虫

### 1. 线程池使用方法

1. 实例化线程池对象

   ```python
    from multiprocessing.dummy import Pool
    pool = Pool(process=5) # 默认大小是cup的个数
   ```

2. 把从发送请求，提取数据，到保存合并成一个函数，交给线程池异步执行

   使用方法`pool.apply_async(func)`

   ```python
    def exetute_requests_item_save(self):
        url = self.queue.get()
        html_str = self.parse_url(url)
        content_list = self.get_content_list(html_str)
        self.save_content_list(content_list)
        self.total_response_num +=1

    pool.apply_async(self.exetute_requests_item_save)
   ```

3. 添加回调函数

   通过`apply_async`的方法能够让函数异步执行，但是只能够执行一次

   为了让其能够被反复执行，通过添加回调函数的方式能够让_callback 递归地调用自己

   同时需要指定递归退出的条件

   ```python
    def _callback(self,temp):
        if self.is_running:
             pool.apply_async(self.exetute_requests_item_save,callback=self._callback)

    pool.apply_async(self.exetute_requests_item_save,callback=self._callback)
   ```

4. 确定程序结束的条件 程序在获取的响应和url数量相同的时候可以结束

   ```python
    while True: #防止主线程结束
        time.sleep(0.0001)  #避免cpu空转，浪费资源
        if self.total_response_num>=self.total_requests_num:
            self.is_running= False
            break
    self.pool.close() #关闭线程池，防止新的线程开启
   # self.pool.join() #等待所有的子线程结束
   ```



### 2. 使用线程池实现爬虫的具体实现

```python
# coding=utf-8
import requests
from lxml import etree
from queue import Queue
from multiprocessing.dummy import Pool
import time


class QiubaiSpider:
    def __init__(self):
        self.url_temp = "https://www.qiushibaike.com/8hr/page/{}/"
        self.headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X \
        10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36"}
        self.queue = Queue()
        self.pool = Pool(5)
        self.is_running = True
        self.total_requests_num = 0
        self.total_response_num = 0

    def get_url_list(self):  # 获取url列表
        for i in range(1, 14):
            self.queue.put(self.url_temp.format(i))
            # ------ 请求数+1 ------
            self.total_requests_num += 1

    def parse_url(self, url):  # 发送请求，获取响应
        return requests.get(url, headers=self.headers).content.decode()

    def get_content_list(self, html_str):  # 提取段子
        html = etree.HTML(html_str)
        div_list = html.xpath("//div[@id='content-left']/div")
        content_list = []
        for div in div_list:
            content = {}
            content["content"] = div.xpath(".//div[@class='content']/span/text()")
            print(content)
            content_list.append(content)
        return content_list

    def save_content_list(self, content_list):  # 保存数据
        pass

    def exetute_requests_item_save(self):
        url = self.queue.get()
        html_str = self.parse_url(url)
        content_list = self.get_content_list(html_str)
        self.save_content_list(content_list)
        # ------ 响应数+1 ------
        self.total_response_num += 1

    def _callback(self, temp):
        if self.is_running:
            self.pool.apply_async(self.exetute_requests_item_save, callback=self._callback)

    def run(self):
        # 1. 准备url列表
        self.get_url_list()
		
        # 2. 请求
        # 3. 提取
        # 4. 保存
        for i in range(3):  # 控制并发数
            self.pool.apply_async(self.exetute_requests_item_save, callback=self._callback)

        while True:  # 防止主线程提前结束
            time.sleep(0.0001)  # 避免cpu空转，浪费资源
            if self.total_response_num >= self.total_requests_num:
                self.is_running = False
                break

        self.pool.close()  # 关闭线程池，防止新的线程开启
        # self.pool.join() # 等待所有的子线程结束

if __name__ == '__main__':
    qiubai = QiubaiSpider()
    qiubai.run()
```



### 3. 使用协程池实现爬虫的具体实现

使用协程池相比线程池,  可以实现更快的爬取

```python
# coding=utf-8

#-------------------------------------------------------------
import gevent.monky
gevent.monky.path_all()  # !! 必须在 import requests 之前打补丁
from gevent.pool import Pool  # 导入协程池的 Pool 类
#-------------------------------------------------------------

import requests
from lxml import etree
from queue import Queue
import time


class QiubaiSpider:
    def __init__(self):
        self.url_temp = "https://www.qiushibaike.com/8hr/page/{}/"
        self.headers = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X \
        10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36"}
        self.queue = Queue()
        self.pool = Pool(5)
        self.is_running = True
        self.total_requests_num = 0
        self.total_response_num = 0

    def get_url_list(self):  # 获取url列表
        for i in range(1, 14):
            self.queue.put(self.url_temp.format(i))
            self.total_requests_num += 1

    def parse_url(self, url):  # 发送请求，获取响应
        return requests.get(url, headers=self.headers).content.decode()

    def get_content_list(self, html_str):  # 提取段子
        html = etree.HTML(html_str)
        div_list = html.xpath("//div[@id='content-left']/div")
        content_list = []
        for div in div_list:
            content = {}
            content["content"] = div.xpath(".//div[@class='content']/span/text()")
            print(content)
            content_list.append(content)
        return content_list

    def save_content_list(self, content_list):  # 保存数据
        pass

    def exetute_requests_item_save(self):
        url = self.queue.get()
        html_str = self.parse_url(url)
        content_list = self.get_content_list(html_str)
        self.save_content_list(content_list)
        self.total_response_num += 1

    def _callback(self, temp):
        if self.is_running:
            self.pool.apply_async(self.exetute_requests_item_save, callback=self._callback)

    def run(self):
        self.get_url_list()

        for i in range(2):  # 控制并发
            
            # ---------- 协程池Pool实例对象也有 apply_async 方法 ----------
            self.pool.apply_async(self.exetute_requests_item_save, callback=self._callback)

        while True:  # 防止主线程结束
            time.sleep(0.0001)  # 避免cpu空转，浪费资源
            if self.total_response_num >= self.total_requests_num:
                self.is_running = False
                break


if __name__ == '__main__':
    qiubai = QiubaiSpider()
    qiubai.run()
```










