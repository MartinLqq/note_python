# 爬虫提高

- selenium
- mongodb数据库
- scrapy框架
- scrapy_redis框架



# 1 selenium

## 1.1 常见的反爬措施和解决思路

### 反反爬的主要思路

- 尽可能的去模拟浏览器，浏览器如何操作，代码中就如何去实现。
- 浏览器先请求了地址url1，保留了cookie在本地，之后请求地址url2，带上了之前的cookie，代码中也可以这样去实现。
- 很多时候，爬虫中携带的headers字段，cookie字段，url参数，post的参数很多，不清楚哪些有用，哪些没用的情况下，只能够去尝试，因为每个网站都是不相同的。当然在盲目尝试之前，可以参考别人的思路，自己也应该有一套尝试的流程。



### 通过headers中字段来反爬

**<1> User-Agent字段**

- 准备User-Agent池

### > 随机生成User-Agent的代码

```python
import random

def get_ua():
    # chrome版本中的数字
    first_num = random.randint(55, 62)
    third_num = random.randint(0, 3200)
    fourth_num = random.randint(0, 140)
    
    # 操作系统类型
    os_type = [
        '(Windows NT 6.1; WOW64)', '(Windows NT 10.0; WOW64)', '(X11; Linux x86_64)',
        '(Macintosh; Intel Mac OS X 10_12_6)'
    ]
    
    # chrome版本
    chrome_version = 'Chrome/{}.0.{}.{}'.format(first_num, third_num, fourth_num)
	
    # 拼接User-Agent
    ua = ' '.join(['Mozilla/5.0', random.choice(os_type), 'AppleWebKit/537.36',
                   '(KHTML, like Gecko)', chrome_version, 'Safari/537.36']
                  )
    return ua

if __name__ == "__mani__":
    ua = get_ua()
   	print(ua)
```



**<2> referer字段或其他字段**

**<3> 通过cookie来反爬**

- 不需要登录,  每次请求带上前一次返回的cookie，比如requests模块的session
- 需要登录,  准备多个账号，通过一个程序获取账号对应的cookie，组成cookie池，其他程序使用这些cookie


### 通过js来反爬

**<1> 通过js实现跳转来反爬**

- 在请求目标网站的时候，我们看到的似乎就请求了一个网站，然而实际上在成功请求目标网站之前，中间可能有通过js实现的跳转，我们肉眼不可见，这个时候可以通过点击perserve log按钮实现观察页面跳转情况
- 在这些请求中，如果请求数量很多，一般来讲，只有那些response中带cookie字段的请求是有用的，意味着通过这个请求，对方服务器有设置cookie到本地

**<2> 通过js生成了请求参数**

- 对应的需要分析js，观察加密的实现过程
- 使用 selenium 很容易就解决这个问题

**<3> 通过js实现了数据的加密**

- 对应的需要分析js，观察加密的实现过程 在下一小节
- 使用 selenium 很容易就解决这个问题



### 通过验证码来反爬

- 通过打码平台或者是机器学习的方法识别验证码
- 打码平台廉价易用，更值得推荐



### 通过ip地址来反爬

- 同一个ip大量请求了对方服务器，有更大的可能性会被识别为爬虫
- 代理ip池



### 其他的反爬方式

**<1> 通过自定义字体来反爬**

如猫眼电影电脑版页面:

![使用字体来反爬](.\02_爬虫提高_images\使用字体来反爬.png)



- **解决思路**：切换到手机版



**<2> 通过css来反爬**

如猫眼去哪儿电脑版: 

![通过css反爬](.\02_爬虫提高_images\通过css反爬.png)

- 电脑版490对应页面上是891,  其中两个数字8把4覆盖了, 1把0覆盖了.	
  - 解决思路之一：计算css的偏移
  - 解决思路之二：切换成手机版
- 手机版490对应页面上也是另一个数,  不过不是css偏移的结果,  而是数字的偏移结果(不是按123456789的顺序)




## 1.2 selenium模块控制浏览器

>  Selenium with Python中文翻译文档		https://selenium-python-zh.readthedocs.io/en/latest/index.html

>  淘宝 NPM 镜像,  下载对应平台对应版本的selenium	https://npm.taobao.org/

### selenium, PhantomJS,  Chromedriver  

Selenium 是一个Web的自动化测试工具，最初是为网站自动化测试而开发的，Selenium 可以直接运行在浏览器上，它支持所有主流的浏览器（包括PhantomJS这些无界面的浏览器），可以接收指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏.

- PhantomJS 是一个基于Webkit的“无界面”(headless)浏览器，它会把网站加载到内存并执行页面上的 JavaScript,   但最新版本的selenium已经不支持PhantomJS 

  - 下载地址：[http://phantomjs.org/download.html](http://phantomjs.org/download.html)

- Chromedriver 也是一个能够被selenium驱动的浏览器，但是和PhantomJS的区别在于Chromedriver 有界面

  - 下载地址：[https://npm.taobao.org/mirrors/chromedriver](https://npm.taobao.org/mirrors/chromedriver)
  - **linux上使用chromedriver**:   最简单的安装方式是：解压后把bin目录下的可执行文件移动到环境变量下，如 `/usr/bin` 或 `/usr/local/bin`
  - **windows上使用chromedriver**:  把chromedriver.exe放在.py文件目录下;  或在实例化Chrome对象时传入chromedriver.exe的路径; 
  - **注意**：Chromedriver和chrome浏览器版本有对应关系，建议使用最新的Chromedriver版本并且更新chrome浏览器到最新版



### selenium入门

发送请求，加载网页
元素定位
获取浏览器elements数据
使用Selenium，处理JavaScript不再是难事




- 加载网页： selenium控制浏览器，获取的数据都是elements中的内容

  ```python
  from selenium import webdriver 
  # driver = webdriver.PhantomJS(“c:…/pantomjs.exe”)
  driver = webdriver.Chrome()			# 声明浏览器对象,  如谷歌Chrome/火狐Firefox/...
  driver.get("http://www.baidu.com/")	# 发起请求
  driver.maxmize_window()				# 窗口最大化
  driver.set_window_size()			# 设置窗口宽高
  driver.save_screenshot("长城.png")   # 保存当前页面截图 / get_screenshot_as_file() 保存截图到文件
  driver.current_url()				# 当前url地址
  ```

- 定位和操作：

  ```python
    driver.find_element_by_id('kw').send_keys('python')
    driver.find_element_by_id('su').click()
  ```

- 查看请求信息：

  ```python
    driver.page_source	# 获取当前页面html源码
    driver.get_cookies()	# 获取当前页面的cookies
    driver.current_url	# 获取当前页面的url
  ```

- 退出

  ```python
    driver.close() #退出当前页面
    driver.quit()  #退出浏览器
  ```




#### > selenium无界面的用法

![selenium无界面用法](.\02_爬虫提高_images\selenium无界面用法.png)



### selenium的定位操作

#### 定位元素

```python
获取多个节点的方法:
	find_elements_by_xpath() 			 	**返回一个包含元素的列表  没有会返回空列表, 不会报错
    find_elements_by_link_text() 		 	**根据连接文本获取元素列表, 如找 "下一页>"  "后页>"
    find_elements_by_partial_link_text() 	**根据连接包含的文本获取元素列表, 如找 包含"下一页"  "后页"
    find_elements_by_name()
    find_elements_by_tag_name()
    find_elements_by_class_name() 	 		**根据类名获取元素列表
	find_elements_by_css_selector()
    
获取单个节点的方法:
    find_element_by_id() 				 	返回一个元素,  没有会报错
    find_element_by_name()
    find_element_by_xpath()
    find_element_by_link_text()
    find_element_by_partial_link_text()
    find_element_by_tag_name()
    find_element_by_class_name()
    find_element_by_css_selector()

通用方法:
    find_element()						传入两个参数：查找方式By和值
    find_elements()
```

**注意**：`by_link_text`和`by_partial_link_tex`的区别：全部文本和包含某个文本



- 使用：以豆瓣首页为例:[https://www.douban.com/](https://www.douban.com/)

  ```python
    from selenium import webdriver

    driver =webdriver.Chrome()

    driver.get("https://www.douban.com/")

    ret1 = driver.find_element_by_id("anony-nav")
    print(ret1)
    # 输出为：<selenium.webdriver.remote.webelement.WebElement (session="ea6f94544ac3a56585b2638d352e97f3", element="0.5335773935305805-1")>

    ret2 = driver.find_elements_by_id("anony-nav")
    print(ret2)
    # 输出为：[<selenium.webdriver.remote.webelement.WebElement (session="ea6f94544ac3a56585b2638d352e97f3", element="0.5335773935305805-1")>]

    ret3 = driver.find_elements_by_xpath("//*[@id='anony-nav']/h1/a")
    print(len(ret3))
     # 输出为：1

    ret4 = driver.find_elements_by_tag_name("h1")
    print(len(ret4))
     # 输出为：1

    ret5 = driver.find_elements_by_link_text("下载豆瓣 App")
    print(len(ret5))
     # 输出为：1

    ret6 = driver.find_elements_by_partial_link_text("豆瓣")
    print(len(ret6))
     # 输出为：28

    driver.close()
  ```
  ​

#### **获取属性/文本**

- find_element 的一系列方法 仅仅能够获取`元素`， 不能直接获取其中的 数据，如 `find_element_by_xapth()`
- 获取数据需要通过以下方法:
  - **获取文本**：`element.text`
  - **获取属性值**：`element.get_attribute("href")`


```python
from selenium import webdriver

driver =webdriver.Chrome()
driver.get("https://www.douban.com/")

ret4 = driver.find_elements_by_tag_name("h1")
print(ret4[0].text)
# 输出：豆瓣

ret5 = driver.find_elements_by_link_text("下载豆瓣 App")
print(ret5[0].get_attribute("href"))
# 输出：https://www.douban.com/doubanapp/app?channel=nimingye

driver.close()
```



**获取元素的 坐标 / 尺寸**

```python
driver.get("http://www.baidu.com")
element = driver.find_element_by_id('su')
x_y = element.location		# {'x': 902, 'y': 209}
size = element.size			# {'height': 36.0, 'width': 100.0}
```





#### 节点交互

Selenium可以驱动浏览器来执行一些操作,  常见的用法有：

- **输入文字**:  `send_keys()`

- **清空文字**:  `clear()`

- **点击按钮**:  `click()`

  ​





#### 获取id、位置、标签名和大小

另外，`WebElement`节点还有一些其他属性，比如`id`属性可以获取节点`id`，`location`属性可以获取该节点在页面中的相对位置，`tag_name`属性可以获取标签名称，`size`属性可以获取节点的大小，也就是宽高，这些属性有时候还是很有用的



更多: 

官方文档的交互动作介绍:   WebDriver API   https://selenium-python.readthedocs.io/api.html#webdriver-api

is_selected()			Returns whether the element is selected
screenshot()			获取网页截图
screenshot_as_base64	Gets the screenshot of the current element as a base64 encoded string
​						Usage:  element_png = element.screenshot_as_png
submit()				提交表单



### selenium 处理cookie

通过`driver.get_cookies()`能够获取所有的cookie

```python
# 把get_cookies方法返回的数据转化为字典 (-----只提取需要的cookie, 交给requests模块使用-----)
{cookie['name']: cookie['value'] for cookie in driver.get_cookies()}	# 使用字典推导式

driver.delete_cookie("CookieName")	# 删除一条cookie
driver.delete_all_cookies()    		# 删除所有的cookie
```



### 延时等待

- 为什么需要等待 -------->  等待浏览器拿到页面资源,  selenium再提取数据

  在Selenium中，`get()`方法会在网页框架加载结束后结束执行，此时如果获取`page_source`，可能并不是浏览器完全加载完成的页面，如果某些页面有额外的Ajax请求，我们在网页源代码中也不一定能成功获取到。所以，这里需要延时等待一定时间，确保节点已经加载出来。

- 显式等待

  - 显式等待是你在代码中定义等待一定条件发生后再进一步执行你的代码。
  - 最糟糕的案例是使用time.sleep()，它将条件设置为等待一个确切的时间段。 
  - 这里有一些方便的方法让你只等待需要的时间。`WebDriverWait` 结合 `ExpectedCondition` 是实现的一种方式。

  ```python
  from selenium import webdriver
  from selenium.webdriver.common.by import By
  from selenium.webdriver.support.ui import WebDriverWait
  from selenium.webdriver.support import expected_conditions as EC

  driver = webdriver.Firefox()
  driver.get("http://somedomain/url_that_delays_loading")
  try:
      element = WebDriverWait(driver, 10).until(
          EC.presence_of_element_located((By.ID, "myDynamicElement"))
      )
  finally:
      driver.quit()
  ```

  在抛出TimeoutException异常之前将等待10秒或者在10秒内发现了查找的元素。 WebDriverWait 默认情况下会每500毫秒调用一次ExpectedCondition直到结果成功返回。 ExpectedCondition成功的返回结果是一个布尔类型的true或是不为null的返回值。

- 隐式等待

  - 如果某些元素不是立即可用的，隐式等待是告诉WebDriver去等待一定的时间后去查找元素。 
  - 默认等待时间是0秒，一旦设置该值，隐式等待是设置该WebDriver的实例的生命周期。

  ```python
  from selenium import webdriver

  driver = webdriver.Firefox()
  driver.implicitly_wait(10)  # seconds
  driver.get("http://somedomain/url_that_delays_loading")
  myDynamicElement = driver.find_element_by_id("myDynamicElement")
  ```




### 使用selenium切换frame

- frame是html中常用的一种技术，即一个页面中嵌套了另一个网页，selenium默认是访问不了frame中的内容的，对应的解决思路是 `driver.switch_to.frame("frame name or id")`

- switch_to的用法: 


  ```python
  element = driver.switch_to.active_element
  alert = driver.switch_to.alert
  driver.switch_to.default_content()
  driver.switch_to.frame('frame_name')
  driver.switch_to.frame(1)
  driver.switch_to.frame(driver.find_elements_by_tag_name("iframe")[0])
  driver.switch_to.parent_frame()
  driver.switch_to.window('main')
  ```

  

- 动手：模拟登陆qq邮箱
  - 在使用selenium登录qq邮箱的过程中，我们会发现，无法在邮箱的登录input标签中输入内容，通过观察源码可以发现，form表单在一个frame中，所以需要切换到frame中




### 前进和后退

平常使用浏览器时都有前进和后退功能，Selenium也可以完成这个操作

- `driver.back()`			后退
- `driver.forward()`		前进



### 选项卡管理

在访问网页的时候，会开启一个个选项卡。在Selenium中，我们也可以对选项卡进行操作

```python
import time
from selenium import webdriver

browser = webdriver.Chrome()
browser.get('https://www.baidu.com')
browser.execute_script('window.open()')    # 传入window.open()这个JavaScript语句新开启一个选项卡
print(browser.window_handles)    # 调用window_handles属性获取当前开启的所有选项卡，返回的是选项卡的代号列表
browser.switch_to_window(browser.window_handles[1])  # 调用switch_to_window()方法切换选项卡
browser.get('https://www.taobao.com')
time.sleep(1)
browser.switch_to_window(browser.window_handles[0])
browser.get('https://python.org')
```





### selenium的优缺点

- selenium能够执行页面上的js，对于js渲染的数据和模拟登陆处理起来非常容易
- selenium由于在获取页面的过程中会发送很多请求，所以效率非常低，所以在很多时候需要酌情使用




### 网易云音乐列表

爬取网易云音乐的所有分类下的所有的播放列表,   地址：http://music.163.com/#/discover/playlist

**思路分析：**

​	<1> 使用requests模块请求起始 url , 获取html 

​	<2> 使用etree模块 和 xpath语法 提取 html 的分类数据,  包括大分类(语种, 风格, ...) 和 小分类(华语, 欧美, ...)

​	<3> 循环获取每个小分类下的playlist列表

​	<4> 获取每个小分类下的每个播放列表中的所有歌曲信息,  同时使用selenium模块获取每个歌单的歌曲信息

```python
# coding=utf-8
import requests
from lxml import etree
import re
from selenium import webdriver
from copy import deepcopy

class Music163:
    def __init__(self):
        self.start_url = "http://music.163.com/discover/playlist"
        self.headers = {"User-Agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36"}
    def parse_url(self,url):
        print(url)
        resp = requests.get(url,headers=self.headers)
        return resp.content.decode()

    def get_category_list(self):  #获取大分类和小分类
        resp = self.parse_url(self.start_url)
        html = etree.HTML(resp)
        dl_list = html.xpath("//div[@class='bd']/dl")
        category_list = []
        for dl in dl_list:
            b_cate = dl.xpath("./dt/text()")[0] if len(dl.xpath("./dt/text()"))>0 else None
            a_list = dl.xpath("./dd/a")
            for a in a_list:
                item = {}
                item["b_cate"]= b_cate
                item["s_cate"] = a.xpath("./text()")[0] if len(a.xpath("./text()"))>0 else None
                item["s_href"] = "http://music.163.com" + a.xpath("./@href")[0] if len(a.xpath("./@href"))>0 else None
                category_list.append(item)
        return category_list

    def get_playlist_list(self,item,total_playlist_list):#获取小分类中的playlist列表
        playlist_list = []
        if item["s_href"] is not None:
            scate_resp = self.parse_url(item["s_href"])
            scate_html = etree.HTML(scate_resp)
            li_list = scate_html.xpath("//ul[@id='m-pl-container']/li")
            for li in li_list:
                item["playlist_title"] = li.xpath("./p[@class='dec']/a/@title")[0] if len(li.xpath("./p[@class='dec']/a/@title"))>0 else None
                print(item["playlist_title"])
                item["playlist_href"] = "http://music.163.com"+li.xpath("./p[@class='dec']/a/@href")[0] if len(li.xpath("./p[@class='dec']/a/@href"))>0 else None
                item["author_name"] = li.xpath("./p[last()]/a/@title")[0] if len(li.xpath("./p[last()]/a/@title"))>0 else None
                item["author_href"] = "http://music.163.com"+li.xpath("./p[last()]/a/@href")[0] if len(li.xpath("./p[last()]/a/@href"))>0 else None
                playlist_list.append(deepcopy(item))
            total_playlist_list.extend(playlist_list)
            next_url = scate_html.xpath("//a[text()='下一页']/@href")[0] if len(scate_html.xpath("//a[text()='下一页']/@href"))>0 else None
            if next_url is not None and next_url!='javascript:void(0)':
                item["s_href"] = "http://music.163.com"+next_url
                #递归，调用自己，获取下一页的播放列表，直到下一页没有的时候不再递归
                return self.get_playlist_list(item,total_playlist_list)
        return total_playlist_list

    def get_playlist_info(self,playlist): #获取单个播放别表的信息
        if playlist["playlist_href"] is not None:
            playlist_resp = self.parse_url(playlist["playlist_href"])
            playlist["covers"] = re.findall("\"images\": .*?\[\"(.*?)\"\],",playlist_resp)
            playlist["covers"] =  playlist["covers"][0] if len(playlist["covers"])>0 else None
            playlist["create_time"] = re.findall("\"pubDate\": \"(.*?)\"",playlist_resp)
            playlist["create_time"] = playlist["create_time"][0] if len(playlist["create_time"])>0 else None
            playlist_html = etree.HTML(playlist_resp)
            playlist["favorited_times"] = playlist_html.xpath("//a[@data-res-action='fav']/@data-count")[0] if len(playlist_html.xpath("//a[@data-res-action='fav']/@data-count"))>0 else None
            playlist["shared_times"] = playlist_html.xpath("//a[@data-res-action='share']/@data-count")[0] if len(playlist_html.xpath("//a[@data-res-action='share']/@data-count"))>0 else None
            playlist["desc"] = playlist_html.xpath("//p[@id='album-desc-dot']/text()")
            playlist["played_times"] = playlist_html.xpath("//strong[@id='play-count']/text()")[0] if len(playlist_html.xpath("//strong[@id='play-count']/text()"))>0 else None
            playlist["tracks"] = self.get_playlist_tracks(playlist["playlist_href"])
            return playlist

    def get_playlist_tracks(self,href): #获取每个歌单的歌曲信息
        driver = webdriver.Chrome()
        driver.get(href)
        driver.switch_to.frame("g_iframe")
        tr_list = driver.find_elements_by_xpath("//tbody/tr")
        playlist_tracks = []
        for tr in tr_list:
            track = {}
            track["name"] = tr.find_element_by_xpath("./td[2]//b").get_attribute("title")
            track["duration"] = tr.find_element_by_xpath("./td[3]/span").text
            track["singer"] = tr.find_element_by_xpath("./td[4]/div").get_attribute("title")
            track["album_name"] = tr.find_element_by_xpath("./td[5]//a").get_attribute("title")
            playlist_tracks.append(track)
        driver.quit()
        return playlist_tracks


    def run(self):
        categroy_list = self.get_category_list() #获取分类
        for cate in categroy_list:
            total_playlist_list = self.get_playlist_list(cate,[]) #获取每个分类下的所有播放列表
            print("-"*100)
            print(total_playlist_list)
            print("-"*100)
            for playlist in total_playlist_list:
                print(playlist,"*"*100)
                playlist = self.get_playlist_info(playlist)  #获取每个播放列表下的所有歌曲信息
                print(playlist)

if __name__ == '__main__':
    music_163 = Music163()
    music_163.run()
```





-----------------------没有对比没有差距,  以下爬虫效率低-----------------------

```python
import json
import time
from lxml import etree

from selenium import webdriver
import requests


class NetEaseCloudMusic(object):
    """ 完成网易云音乐的所有分类下的所有的播放列表
        包括：播放列表的标题和url地址
        ----> 并没有获取每首歌的信息 <----
    """

    def __init__(self):
        self.start_url = "https://music.163.com/#/discover/playlist"
        self.driver = webdriver.Firefox()

    def cat_name_url_list(self):
        self.driver.get(self.start_url)
        # 切换到iframe
        self.driver.switch_to.frame("g_iframe")
        # 打开分类框
        self.driver.find_element_by_id('cateToggleLink').click()
        element_li = self.driver.find_elements_by_xpath("//dl[@class='f-cb']/dd//a")
        cat_name_url_list = list()
        for cat in element_li:
            item = dict()
            item['cat_name'] = cat.text
            item['cat_url'] = cat.get_attribute("href")
            cat_name_url_list.append(item)
        # 关闭分类框
        self.driver.find_element_by_id('cateToggleLink').click()
        print(cat_name_url_list)
        print('='*100)
        return cat_name_url_list

    def click_cat(self, cat_name):
        # self.driver.switch_to.frame("g_iframe")
        self.driver.find_element_by_id('cateToggleLink').click()
        # 点击分类进入音乐列表
        self.driver.find_element_by_xpath("//dl[@class='f-cb']/dd//a[contains(@data-cat, '{}')]"
                                          .format(cat_name)).click()

    def get_content_list(self, content_list):
        # 获取当前页播放列表
        obj_list = self.driver.find_elements_by_xpath("//ul[@id='m-pl-container']/li")
        for obj in obj_list:
            item = dict()
            item['title'] = obj.find_element_by_xpath("./p[1]/a").text
            item['play_url'] = obj.find_element_by_xpath("./p[1]/a").get_attribute("href")
            item['by'] = obj.find_element_by_xpath(".//span[@class='nb']").text
            print(item)
            content_list.append(item)
        # ---- 滚动到页面底部 ----
        self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        # 点击下一页
        next_page = self.driver.find_elements_by_xpath("//div[@id='m-pl-pager']//a[contains(text(), '下一页')]")
        # 判断是否还可以点击下一页
        if next_page[0].get_attribute("class") == "zbtn znxt":
            next_page[0].click()
            # 递归, 获取下一页的播放列表
            return self.get_content_list(content_list)
        else:
            return content_list

    def save_content_list(self, cat_name, content_list):
        json_str = json.dumps(content_list, ensure_ascii=False, indent=2)

        path = r"C:\Users\Administrator\Desktop\new\爬虫提高\results\\"
        file_name = path + '[网易云音乐]-' + cat_name + '.json'
        with open(file_name, 'w', encoding="utf-8") as f:
            f.write(str(json_str))
        print("-----------saved one-----------")

    def run(self):
        # 音乐分类的url_list
        cat_name_url_list = self.cat_name_url_list()
        # 遍历, 点击每个分类
        for cat_dict in cat_name_url_list:
            cat_name = cat_dict['cat_name']
            # 点击分类
            self.click_cat(cat_name)
            time.sleep(1)
            # 获取当前分类下的播放列表
            content_list = self.get_content_list([])
            # 保存
            self.save_content_list(cat_name, content_list)
        # 关闭浏览器
        time.sleep(10)
        self.driver.quit()


if __name__ == '__main__':
    music = NetEaseCloudMusic()
    music.run()
```





### + 元素操作之鼠标操作

#### > 动作链 ActionChains

- 在上面的实例中，一些交互动作都是针对某个节点执行的。比如，对于 `输入框`，我们就调用它的输入文字和清空文字方法；对于`按钮`，就调用它的点击方法;
- 还有另外一些操作，它们没有特定的执行对象，比如`鼠标拖曳`、`键盘按键`等，这些动作用另一种方式来执行，那就是**动作链**.



**ActionChains提供的方法**

```python

以下方法是 ActionChains 类的 实例对象的方法

click(on_element=None)				  单击鼠标左键
click_and_hold(on_element=None)		  点击鼠标左键，按住不放
context_click(on_element=None)		  点击鼠标右键
double_click(on_element=None)		  双击鼠标左键
drag_and_drop(source, target)		  拖拽到某个元素然后松开
drag_and_drop_by_offset(source, xoffset, yoffset)	  拖拽到某个坐标然后松开
move_by_offset(xoffset, yoffset)	  鼠标移动到距离当前位置（x,y）
move_to_element(to_element)			  鼠标移动到某个元素
move_to_element_with_offset(to_element, xoffset, yoffset)	  将鼠标移动到距某个元素多少距离的位置
release(on_element=None)			  在某个元素位置松开鼠标左键

perform()							  执行链中的所有动作

```



比如，现在实现一个节点的拖曳操作，将某个节点从一处拖曳到另外一处:

```python
# ------ 实现节点的拖曳操作 ------

from selenium import webdriver
from selenium.webdriver import ActionChains

driver = webdriver.Firefox()
url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'
driver.get(url)
driver.switch_to.frame('iframeResult')    # 打开网页中的一个拖曳实例
source = driver.find_element_by_css_selector('#draggable')  # 选中要拖曳的节点
target = driver.find_element_by_css_selector('#droppable')  # 选中拖曳到的目标节点
actions = ActionChains(driver)			  # 声明ActionChains对象并将其赋值为actions变量
actions.drag_and_drop(source, target)     # 调用actions变量的drag_and_drop()方法

actions.perform()	 # 调用perform()方法执行动作
```



### + 元素操作之键盘操作

```python
# 导入Keys 模块，然后看看Keys 模块定义了那些按键
from selenium.webdriver.common.keys import Keys
```

#### > 组合键

```python
send_keys(Keys.CONTROL,'a') 　　全选（Ctrl+A）
send_keys(Keys.CONTROL,'c') 　　复制（Ctrl+C）
send_keys(Keys.CONTROL,'x') 　　剪切（Ctrl+X）
send_keys(Keys.CONTROL,'v') 　　粘贴（Ctrl+V）
```

#### > 非组合键

```python
Keys.ENTER  		回车键  可以尝试使用回车键来实现鼠标的click()键
Keys.BACK_SPACE		删除键 
Keys.SPACE			空格键 
Keys.TAB			制表键 
Keys.ESCAPE			回退键 
Keys.F5				刷新键 
```



### + 填写表格

我们已经知道如何在input或textarea元素中输入内容，但是其他元素怎么办？ 

你可以“切换”下拉框的状态，你可以使用``setSelected``方法去做一些事情，比如 选择下拉列表，处理`SELECT`元素其实没有那么麻烦:

```python
element = driver.find_element_by_xpath("//select[@name='name']")
all_options = element.find_elements_by_tag_name("option")
for option in all_options:
    print("Value is: %s" % option.get_attribute("value"))
    option.click()
```

上面这段代码将会寻找页面第一个 “SELECT” 元素, 并且循环遍历每一个OPTION元素，然后按顺序都选中一遍。

正如你说看到的那样，这不是处理 SELECT 元素最好的方法。

**WebDriver的支持类包括一个叫做Select的类，提供有用的方法处理这些内容**:

```python
from selenium.webdriver.support.ui import Select
select = Select(driver.find_element_by_name('name'))	# 实例化Select对象, 传入element对象
select.select_by_index(index)			# 调用select对象的方法选择 OPTION 元素
select.select_by_visible_text("text")
select.select_by_value(value)
```

取消选择已经选择的元素:

```python
select = Select(driver.find_element_by_id('id'))
select.deselect_all()
```

列出所有已经选择的选项:

```python
select = Select(driver.find_element_by_xpath("xpath"))
all_selected_options = select.all_selected_options
```

获得所有选项:

```python
options = select.options
```



提交表单方法之一:   element对象的click()方法,  找到一个“submit” 按钮然后点击提交表单

```python
# Assume the button has the ID "submit" :)
driver.find_element_by_id("submit").click()
```

提交表单方法之二:   element对象的submit()方法

WebDriver对 **每一个元素** 都有一个叫做 “submit” 的方法，如果你在一个表单内的 元素上使用该方法，WebDriver会在DOM树上就近找到最近的表单，返回提交它。 如果调用的元素不在表单内，将会抛出``NoSuchElementException``异常:

```python
element.submit()
```



### + 执行JavaScript

对于某些操作，Selenium API并没有提供。比如，下拉进度条，它可以直接模拟运行JavaScript，此时用`execute_script()`方法即可实现.

下拉进度条:

```python
from selenium import webdriver

browser = webdriver.Chrome()
browser.get('https://www.zhihu.com/explore')
browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')    # 将进度条下拉到最底部
browser.execute_script('alert("To Bottom")')    # 弹出alert提示框
```

有了这个方法，基本上API没有提供的所有功能都可以用执行JavaScript的方式来实现了。

#### > 如何滚动到页面底部？

你可以在加载完成的页面上使用 execute_script 方法执行js。所以， 你调用javascript API滚动到底部或页面的任何位置。

这里是一个滚动到页面底部的例子:

```python
# 执行 js 脚本
driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
```

该 [window](http://www.w3schools.com/jsref/obj_window.asp) 对象在DOM有一个 [scrollTo](http://www.w3schools.com/jsref/met_win_scrollto.asp) 滚动到打开窗口 的任意位置的方法。 该 [scrollHeight](http://www.w3schools.com/jsref/dom_obj_all.asp) 是所有元素的共同属性。 该 document.body.scrollHeight 将给出整个页面体的高度。



### + 附录：常见问题

Another FAQ: https://github.com/SeleniumHQ/selenium/wiki/Frequently-Asked-Questions

**> 如何使用 ChromeDriver ?**

下载最新版本的 [chromedriver](https://sites.google.com/a/chromium.org/chromedriver/downloads). 解压缩这个文件:

```
unzip chromedriver_linux32_x.x.x.x.zip
```

你应该会看到一个 `chromedriver` 的可执行文件. 现在你可以像这样创建一个 Chrome WebDriver 实例:

```
driver = webdriver.Chrome(executable_path="/path/to/chromedriver")

```

这个示例的其余部分应该在其他的文档中给出。

**> 如果上传文件到文件上传控件？**

选择 `<input type="file">` 元素并且调用 `send_keys()` 方法传入要上传文件的路径，可以 是对于测试脚本的相对路径，也可以是绝对路径。 请牢记在Windows和Unix系统之间的路径名的区别。

**> 如果在Firefox中使用firebug工具？**

首先下载Firebug插件的XPI文件， 然后调用对于firefox 的配置提供的 `add_extension` 方法

```python
from selenium import webdriver

fp = webdriver.FirefoxProfile()

fp.add_extension(extension=’firebug-1.8.4.xpi’) fp.set_preference(“extensions.firebug.currentVersion”, “1.8.4”) #Avoid startup screen browser = webdriver.Firefox(firefox_profile=fp)
```

**> 如何配置使用代理？**

代理配置是通过`org.openqa.selenium.Proxy`类完成的，如下所示：

```python
Proxy proxy = new Proxy();
proxy.setProxyAutoconfigUrl("http://youdomain/config");

// We use firefox as an example here.
DesiredCapabilities capabilities = DesiredCapabilities.firefox();
capabilities.setCapability(CapabilityType.PROXY, proxy);

// You could use any webdriver implementation here
WebDriver driver = new FirefoxDriver(capabilities);
```

**> 为什么无法与隐藏元素进行交互？**

答：由于用户无法读取隐藏元素中的文本，因此WebDriver也不允许访问它。

但是，可以使用Javascript执行功能直接从元素调用getText：





## 1.3 打码平台

现在很多网站都会使用验证码来进行反爬，所以为了能够更好的获取数据，需要了解如何使用打码平台爬虫中的验证码

### 常见的打码平台

1. **云打码**：[http://www.yundama.com/](http://www.yundama.com/)

   能够解决通用的验证码识别

2. **极验验证码**智能识别辅助：[http://jiyandoc.c2567.com/](http://jiyandoc.c2567.com/)

   能够解决复杂验证码的识别



### 云打码API

下面代码是云打码平台提供，做了个简单修改，只用传入response.content 即可识别图片

#### > 云打码官方接口代码(改)

```python

需要修改其中的用户相关配置

import requests
import json
import time

class YDMHttp:
    apiurl = 'http://api.yundama.com/api.php'
    username = ''
    password = ''
    appid = ''
    appkey = ''

    def __init__(self, username, password, appid, appkey):
        self.username = username
        self.password = password
        self.appid = str(appid)
        self.appkey = appkey

    def request(self, fields, files=[]):
        response = self.post_url(self.apiurl, fields, files)
        response = json.loads(response)
        return response

    def balance(self):
        data = {'method': 'balance', 'username': self.username, 'password': self.password, 'appid': self.appid,
                'appkey': self.appkey}
        response = self.request(data)
        if (response):
            if (response['ret'] and response['ret'] < 0):
                return response['ret']
            else:
                return response['balance']
        else:
            return -9001

    def login(self):
        data = {'method': 'login', 'username': self.username, 'password': self.password, 'appid': self.appid,
                'appkey': self.appkey}
        response = self.request(data)
        if (response):
            if (response['ret'] and response['ret'] < 0):
                return response['ret']
            else:
                return response['uid']
        else:
            return -9001

    def upload(self, filename, codetype, timeout):
        data = {'method': 'upload', 'username': self.username, 'password': self.password, 'appid': self.appid,
                'appkey': self.appkey, 'codetype': str(codetype), 'timeout': str(timeout)}
        file = {'file': filename}
        response = self.request(data, file)
        if (response):
            if (response['ret'] and response['ret'] < 0):
                return response['ret']
            else:
                return response['cid']
        else:
            return -9001

    def result(self, cid):
        data = {'method': 'result', 'username': self.username, 'password': self.password, 'appid': self.appid,
                'appkey': self.appkey, 'cid': str(cid)}
        response = self.request(data)
        return response and response['text'] or ''

    def decode(self, filename, codetype, timeout):
        cid = self.upload(filename, codetype, timeout)
        if (cid > 0):
            for i in range(0, timeout):
                result = self.result(cid)
                if (result != ''):
                    return cid, result
                else:
                    time.sleep(1)
            return -3003, ''
        else:
            return cid, ''

    def post_url(self, url, fields, files=[]):
        # for key in files:
        #     files[key] = open(files[key], 'rb');
        res = requests.post(url, files=files, data=fields)
        return res.text 

username = 'whoamI' # 用户名

password = '***' # 密码

appid = 4283  # 注册为开发者后平台给的appid

appkey = '02074c64f0d0bb9efb2df455537b01c3'  # 注册为开发者后平台给的appkey

filename = 'getimage.jpg' # 文件位置

codetype = 1004  # 选择验证码类型, 1000表示自动识别位数

# 超时
timeout = 60

def indetify(response_content):
    if (username == 'username'):
        print('请设置好相关参数再测试')
    else:
        # 初始化
        yundama = YDMHttp(username, password, appid, appkey)

        # 登陆云打码
        uid = yundama.login();
        print('uid: %s' % uid)

        # 查询余额
        balance = yundama.balance();
        print('balance: %s' % balance)

        # 开始识别，图片路径，验证码类型ID，超时时间（秒），识别结果
        cid, result = yundama.decode(response_content, codetype, timeout)
        print('cid: %s, result: %s' % (cid, result))
        return result

def indetify_by_filepath(file_path):
    if (username == 'username'):
        print('请设置好相关参数再测试')
    else:
        # 初始化
        yundama = YDMHttp(username, password, appid, appkey)

        # 登陆云打码
        uid = yundama.login();
        print('uid: %s' % uid)

        # 查询余额
        balance = yundama.balance();
        print('balance: %s' % balance)

        # 开始识别，图片路径，验证码类型ID，超时时间（秒），识别结果
        cid, result = yundama.decode(file_path, codetype, timeout)
        print('cid: %s, result: %s' % (cid, result))
        return result

if __name__ == '__main__':
    pass
```



#### > 接口使用步骤

**<1>** 获取验证码图片url		

​	url = driver.get_element_by_id("//xxxx")

**<2>** 请求验证码图片url,  获取二进制响应 

> 假设这种验证码是url地址不变则验证码不变.  而在之前的flask项目中, 多次访问同一个图片验证码url, 返回的图片都不一样

​	response_content = requests.get(url).content

**<3>** 调用接口,  这里是调用我们自己进一步封装的函数  indetify(response_content),  接收验证码识别结果

**<4>** 使用selenium 输入验证码,  点击登录



### 常见的验证码种类

#### > url地址不变，验证码不变

这是验证码里面非常简单的一种类型，对应的只需要获取验证码的地址，然后请求，通过打码平台识别即可

#### > url地址不变，验证码变化

这种验证码的类型是更加常见的一种类型，对于这种验证码，大家需要思考：

> 在登录的过程中，假设用户输入的验证码是对的，对方服务器是如何判断当前用户输入的验证码是显示在用户屏幕上的验证码，而不是其他的验证码呢？

- 在获取网页的时候，请求验证码，以及提交验证码的时候，对方服务器肯定通过了某种手段验证我之前获取的验证码和最后提交的验证码是同一个验证码，那这个手段是什么手段呢？
  - 很明显，就是通过 `cookie` 来实现的
  - 所以对应的，在请求页面，请求验证码，提交验证码的到时候需要保证cookie的一致性，对此可以使用**requests.Session()** 来解决
  - 即:  请求时带上一致的 cookie




#### > selenium处理验证码截屏

- No.1 带上selenium的driver中的cookie来请求验证码
- No.2 selenium获取验证码对应element对象的location属性,  size属性,  selenium截屏

```python
# -*- coding: utf-8 -*-
from selenium import webdriver
from PIL import Image

driver = webdriver.Chrome()
driver.get("https://www.baidu.com")
driver.save_screenshot("baidu01.png")
# print(driver.get_window_size())
element = driver.find_element_by_id("su")  # 定位元素
print(element.location)  # 打印元素坐标
print(element.size)  # 打印元素大小

# 获取图片坐标
left = element.location['x']
top = element.location['y']
right = element.location['x'] + element.size['width']
bottom = element.location['y'] + element.size['height']
print(left, top, right, bottom)  # 图标四个点的坐标

im = Image.open('baidu01.png')
#  crop() : 从图像中提取出某个矩形大小的图像。它接收一个四元素的元组作为参数，各元素为（left, upper, right, lower），坐标系统的原点（0, 0）是左上角。
im = im.crop((left, top, right, bottom))
im.save('bdbutton.png')

driver.quit()
```













